\documentclass[doc,floatsintext,natbib]{apa7}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{doi}
\usepackage{booktabs}
%\usepackage[dvipsnames]{xcolor}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{setspace} % for adjusting line spread
\usepackage{hyperref} 

\renewcommand{\doitext}{} % prevents superfluous addition of "doi:" in references
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % uses symbols instead of numbers for footnotes
\usepackage[colorinlistoftodos, textsize=footnotesize]{todonotes}
%%\todo[inline, color=green!40]{This is a green inline comment.}

%text from Marjolein edited by Caro
\newcommand{\edc}[1]{\textcolor{blue}{#1}}

\linespread{1.3}


\title{One Model May Not Fit All: Subgroup Detection Using MOB}
\shorttitle{shorttitle}

\authorsnames{Marjolein Fokkema$^1$, Mirka Henninger$^2$ \& Carolin Strobl$^2$}
\authorsaffiliations{$^1$Leiden University, $^2$Universit\"at Z\"urich}

\abstract{Model-based recursive partitioning \citep[MOB,][]{ZeilyHoth08} is a flexible framework for detecting subgroups \edc{of persons showing different} effects in a wide range of parametric models. It provides a versatile tool for detecting and explaining heterogeneity of intervention effects. In this tutorial paper, we provide an introduction to the general MOB framework. In two specific case studies, we show how MOB-based methods can be used to detect and explain heterogeneity in two widely-used frameworks in educational studies: mixed-effects and item response theory models. In the first case study, we show how GLMM trees \citep{FokkySmit18} can be used to detect subgroups with different parameters in mixed-effects models. We apply GLMM trees to a dataset from a study of \citep{Demi09}, who compared longitudinal performance of siblings who did, and siblings who did not participate in the Head Start program. Using GLMM trees, we identify subgroups of families in which children show comparatively larger or smaller gains in performance following participation in Head Start. In a second case study, we show how Rasch trees \citep{StroyKopf15} can be used to detect subgroups with different item parameters in IRT models, \edc{i.e.~differential item functioning (DIF).
DIF should be investigated before using test results for comparing groups, because undetected DIF can affect test fairness.} We show how a recently developed stopping criterion \citep{HennyDeba23} can be used to guide subgroup detection based on DIF effect sizes.\\}

\begin{document}
\SweaveOpts{concordance=TRUE}
\renewenvironment{Schunk}{\small}{}

<<echo=FALSE>>=
## suppress printing of > and + before code, it is annoying for copy-pasting
options(prompt=" ", continue = " ") 
@

\maketitle

\todo[inline]{Target journal: \textit{Journal of School Psychology}. This Special Issue is on "Conceptual and Methodological Advances for Understanding Contextual, Identity, and Cultural Effects in Intervention Research". Submission deadline: October 31, 2023}


\newpage
\section{Introduction}
\label{sec:Introduction}

Model-based recursive partitioning \citep[MOB,][]{ZeilyHoth08} is a semi-parametric approach for detecting differences in the parameters of a statistical
model between groups of persons. This method is a generalization of the principle of recursive partitioning, that is also used in classification and regression trees \citep{Breetal:1984}. 
In classification and regression trees, the aim is to detect groups of persons, specified by (combinations of) covariates, that differ in a response variable. An example could be that children who are highly motivated and have good reading skills show higher average math exam grades. 
MOB generalizes this idea by identifying groups of persons, specified by (combinations of) covariates, that differ in a the parameters of a statistical model. An example could be that children who are highly motivated and have good reading skills show higher slopes in a regression model relating the time spent studying to math exam grades. 

The framework of MOB is very flexible and can be applied in combination with a variety of statistical models, such as linear and generalized linear regression \citep{KopAugStr:2013,ZeilyHoth08} or models for paired-comparison data \citep{StrWicZei:2011:JoEaBS}. In this article, we will highlight two such combinations that we consider particularly relevant for research in school psychology: MOB for mixed-effects models \citep{FokkySmit18} and for models from Rasch measurement and Item Response Theory \citep[IRT,][]{StrKopZei:2015:P,KomStrZei:2017:EaPM,HenDebStr:2023:EPM}. Mixed-effects models become relevant whenever data are collected in repeated measures or nested data structures, for example when children are tested at several time points (so that time points are nested in children) and/or when children from different classes from different schools (so that children are nested in classes, which are again nested in schools) participate in a study. In application example 1 we will illustrate how MOB can be used to detect subgroup-specific intervention effects while taking into account the nested data structure. 

While for this first application example we assume that the administered test \todo{should we call a psychological test a test/an instrument/??? to distinguish it from the verb and noun for statistical test? (Mirka: I like the term instrument to distinguish it from a statistical test)} has already been thoroughly validated, for application example 2 we go back in the research process to the point where a new test is administered to a validation sample. Before using the test in our research, we need to make sure that test results are comparable, for example between children of different genders. If certain items pose an advantage or disadvantage to either group, they are said to violate measurement invariance or exhibit differential item functioning (DIF). In order to test for DIF in the framework of Item Response Theory or Rasch modelling \citep{AnthyDiPe16,DebStrZei:2022:CRC,Mall97}, groups of persons are compared with respect to the item parameters for their specific group. This can be done in a way that allows to detect DIF on top of any true group differences in ability. While many standard approaches for testing DIF are limited to comparing typically two pre-specified groups at a time, we will show in application example 2 that MOB more flexibly allows to detect groups of persons with different item parameters in a data-driven 
way. 

In the following section, we will give a short introduction into the algorithm and statistical concepts behind MOB. Readers interested in learning more about its predecessor method, classification and regression trees, are referred to the introduction by \citet{StrMalTut:2009:PM}. An alternative framework for detecting groups of persons with different model parameters is mixture modelling \citep[see, e.g.,][in the context of Rasch modelling]{AyalySant17,FriStrZei:2015:EaPM}. Mixture modelling aims at identifying latent classes of persons with different properties. It can also be combined with covariates. A comparison of both approches is given by \citet{FriStrZei:2014}.


\section{The Algorithm Behind MOB}

What you see displayed in Figures~\ref{fig:lmm_tree}, \ref{fig:math_tree}, and \ref{fig:read_tree}\todo{adjust if raschtree example changes (Mirka: maybe it would be helpful to have a simplified figure of a GLMERtree and a Raschtree early in the manuscript already, so we can refer to it here?)}, which will be explained in detail in the application example sections, is the result of the MOB algorithm for a linear mixed-effects model (Figure~\ref{fig:lmm_tree}) and for a binary Rasch model (Figures~\ref{fig:math_tree} and \ref{fig:read_tree}). These figures share the tree-structure in the upper part, but differ in their lower part. The starting point of the tree structure is at the very top of each tree. The very top of each tree is called the root node, so what you see are actually ``upside down'' trees with the root at the top and the branches at the 
bottom. The root node at the very top contains the entire sample of persons in the data set. From there, the persons are divided into subgroups.

The subgroups are defined by the covariates that are used for splitting and together with cutpoints in those covariates. For example, in Figure~\ref{fig:lmm_tree} for the metric covariates \texttt{AFTQ} and \texttt{Income} a numeric cutpoint has been selected by the MOB algorithm to separate the groups. The algorithm has identified this cutpoint as being the location of the strongest parameter difference between the two resulting groups. The categorical covariate \texttt{Race}, which was coded in three categories in this example, has been divided into two groups of categories: Black and Hispanic vs.~White. For a binary covariate, \todo{like ... -- add name if is there one in the other example?} 
on the other hand, there is only one possible cutpoint, namely between the two categories. While it is also possible to create more than two groups in each split \citep{KimLoh:2001,Qui:1993}, binary splitting algorithms are typically preferred because they lead to more concise trees. 

The parametric model to which the MOB algorithm is applied is visualized in the end nodes at the bottom of each tree. While in Figure~\ref{fig:lmm_tree} each end node contains a linear mixed-effecs model with a group-specific effect for the Head Start intervention, in Figures~\ref{fig:math_tree} and \ref{fig:read_tree} each end node contains the group-specific item parameters of a Rasch model for the respective test items. 
The fact that the Figures display more than one end node already means that one joint mixed-effects or Rasch model was not appropriate to describe the pattern in the data. Figure~\ref{fig:lmm_tree} also shows that, out of the  covariates that were presented to the algorithm (\texttt{AFTQ}, \texttt{Race}, \texttt{Income},\texttt{Mom\_edu\_yrs}, and \texttt{Mom\_height}), only \texttt{AFTQ}, \texttt{Race}, and \texttt{Income} were actually selected for splitting. Variables are selected based on a statistical test for parameter differences, also termed a test for structural change. Structural change is present if, for example, the model parameters systematically differ for children from lower vs.~higher income families. At each node, the algorithm will select the covariate showing the strongest structural change as the next splitting variable. Within this variables, the optimal cutpoint is chosen in a separate step.

The way the variable selection and cutpoint selection steps are separated in modern classification and regression tree and MOB algorithms, including the ones used here, distinguish them from the traditional classification and regression tree algorithms of \citet{Breetal:1984} and \citet{Qui:1993}. The traditional algorithms performed variable and cutpoint selection in one step. However, this leads to an undesirable behavior called variable selection bias. It means that the traditional algorithms prefer variables offering more cutpoints in the selection process -- regardless of their true information content. An algorithm that still has this problem is, for example, the \texttt{rpart} algorithm in R, based on the original CART algorithm by \citet{Breetal:1984}. Modern algorithms for classification and regression trees that have solved this problem are the unbiased approaches \texttt{QUEST} \citep{LohShi:1997}, which is available in SPSS, and \texttt{ctree} \citep{Hotetal:2006}, available in R in the \texttt{party} and \texttt{partykit} packages \citep{partykit:pkg}. The latter forms the basis for all MOB approaches presented in this paper. 

Once there are no more covariates that show a significant structural change in any node, the MOB algorithm stops splitting. In this way, the MOB algorithm selects only those variables that are relevant for distinguishing the groups, i.e., it performs automatic variable selection. Moreover, the trees will not grow as large as possible, but will stop when no more significant structural change is detected. This is the second difference between the modern classification and regression tree and MOB algorithms used here, compared to traditional classification and regression tree algorithms like those of \citet{Breetal:1984} and \citet{Qui:1993}: While the traditional algorithms grew very large trees and then cut them back by means of so called pruning, the framework employed here is based on statistical significance tests and partly also on effect size measures (see application example 2) to stop the trees already in the growing phase when no more significant change can be detected. Other stopping criteria are based on the number of persons in the end nodes. These crieria ensure that the sample sizes in the end nodes are large enough to estimate the statistical model in each end node.\todo{we should discuss this in the application examples; there are some defaults but for models with many parameters it might make sense to increase them}

In summary, the rationale of a MOB algorithm consists of the following steps:

\todo{why are the spaces so big? I didn't change the enumerate command}
\begin{enumerate}
\item The model parameters are first estimated jointly for all persons in the current node, starting with the full sample.
\item Structural change in the model parameters is assessed with respect to each available covariate.
\item If there is significant structural change, groups are split along the covariate with the strongest change and using the optimal cutpoint.
\item Steps 1--3 are repeated recursively in the resulting groups until there is no more significant structural change (or the groups becomes too small).
\end{enumerate}

For more details on the statistical theory behind unbiased classification and regression trees and MOB, see \citet{Hotetal:2006,StrMalTut:2009:PM,StroyKopf15}.  

An important characteristic of classification and regression trees as well as MOB is that the entire structure identified by the trees does not have to be pre-specified by the researcher in a confirmatory way, but is learned from the data in an exploratory way. This is a key feature of the MOB approach that makes it very flexible and distinguishes it from purely parametric approaches, where only those main effects and interactions that are explicitly included in the specification of the model are considered. While there are phases in psychological and educational research where it is very important to specify hypothesis a priori and test them in a confirmatory way, in early stages of research exploratory methods are an important addition to the statistical toolbox for researchers. 


\todo[inline]{\textbf{Previous Papers on IRT and Rasch Models in Journal of School Psychology}

Rasch and IRT models are often mentioned in this journal's papers as being used for scoring school assessments (e.g., Virginia Standards of Learning Assessments;  Oregon Assessment of Knowledge and Skills; Peabody individual achievement test, \cite{Luth92}).

A general introduction to IRT in the journal is provided by \cite{AnthyDiPe16}. \edc{included}
There are also papers describing extensions to mixure IRT models \cite{AyalySant17} \edc{included}
and many-facet Rasch measurement (MFRM) approaches that allow controlling for rater effects \cite{StycyAntr21}. 

\cite{Mall97} assessed DIF by comparing Rasch model item difficulties of WISC-III subtests between 110 severely and profoundly deaf children, 110 matched nonreferred hearing children, and the WISC-III standardization sample (N = 2,200). \edc{included}
}

The general framework of MOB introduced above will now be applied to two types of statistical models that are particularly relevant in school psychology research: Mixed-effects models for repeated measures or nested data structures and measurement models for validating psychological and educational tests. 

\subsection{Using MOB for Subgroup Detection in Mixed-Effects Models}

In mixed-effects models contain two types of effects: fixed effects and random effects. Fixed effects are typically used, for example, to describe the effect of an intervention on the average student performance. Random effects are used to describe the variation in the intercept and slope for, e.g., students in different classes. 

When mixed-effects models are combined with MOB, ...

\todo[inline]{Marjolein please edit and finish}

\subsection{Using MOB for Detecting DIF in Measurement Models}

Before being able to use a psychological or educational test for comparing different groups of persons in a fair way, different assumptions of the measurement model have to be checked. In the framework of IRT and Rasch measurement, test items are typically investigated with respect item misfit, multidimensionality and other violations of the measurement model \citep[cf., e.g.,][for an introduction]{DebStrZei:2022:CRC}. Another assumption that is particularly crucial for the comparability of test scores between groups is measurement invariance. Items that violate measurement invariance by showing different measurement properties for different groups of participants display DIF.

MOB can be used to detect DIF by means of searching for covariates that display structural change in the item parameters of the measurement model. For the Rasch measurement model, the R function for conducting the MOB analysis is the \texttt{raschtree} function from R package \texttt{psychotree} \citep{StroyKopf15} \todo{should we cite the package too or only the paper?} The usage of this function will be illustrated in application example 2. We will see that, just like for the mixed-effects model trees\todo{use same names for methods everywhere}, we need to specify which variables are part of the parametric model. In the case of the Rasch model, this will be the test items. Moreover, we need to specify, which variables are made available to the MOB algorithm for selecting relevant splitting variables and cutpoints. 

If one joint Rasch model holds for the entire sample, i.e., if there is no DIF, a Rasch tree should show no splits. However, in certain settings in educational research, such as large scale assessements, very large sample sizes are available for testing for DIF. Large sample sizes are good for detecting even small effects or model violations with a high statistical power. The same holds for the statistical tests used for detecting parameter change in the MOB algorithm, so that in larger samples even very small parameter differences can be detected with large samples. However, in DIF detection this may mean that even very small DIF effects, that in practice can be considered ignorable, will be detected if only the sample is big enough. As we will illustrate in the first part of application example 2, for a large sample of university students who have taken a quiz to test their general knowledge, this can lead to very large trees, that are hard to interpret and contain splits that would not be considered relevant by measurement experts. Therefore, an extension of Rasch trees has been suggested by \citet{HennyDeba23} based on the Mantel-Haenszel effect size measure for DIF. \citet{HolTha:1985} have suggested an intuitive classification of DIF effect sizes based in the Mantel-Haenszel statistic, that is being widely used in educational testing. In this classification, category A stands for negligible DIF (small effect size or not statistically significant), B for medium DIF (neither A nor C), and C for large DIF (large effect size and statistically significant). \citet{HennyDeba23} have incorporated this classification as an additional stopping criterion for Rasch trees, so that the user can decide, for example, that a split should only be conducted if the detected DIF is of category B or C, while negligible DIF of category A should be ignored. 

As we will show in application example 2, for large sample sizes this can be very helpful because it results in shorter trees that are easier to interpret and contain only splits corresponding to DIF effect sizes considered relevant in practice. Together with a purification step \citep[see][for details]{HennyDeba23}, the Mantel-Haenszel classification can also be used for highlighting those items that show DIF with respect to certain groups of persons graphically. This can help generate hypothesis about the sources of DIF, as we will ilustrate in application example 2, and can also aid the decision how to proceed with the DIF items. 
\todo[inline]{following text from Caro can be used here or later after example 2 or in discussion}
For example, items that show DIF between different language groups can often be improved by means of making sure that in all translations the meaning is as similar as possible, that the words employed in the translations for the different languages are equally frequently used, etc. In other situations, sources of DIF might be harder to eliminate, so that often DIF items are excluded from a test. Either way, the measurement model needs to be refitted and its assumptions checked again after the final set of items has been decided upon and, in the case of modified items, administered again. 

Another important aspect to keep in mind is that DIF can be caused by one or more items measuring a secondary dimension in addition to the dimension that is intended to be measured by the test. This would be the case in instruments intended to measure math aptitude containing pure algebra problems as well as story problems. Students whose native language is not the same as the test language can have a disadvantage in answering the story problems, for example when they contain seldomly used words. These items will then show DIF between native and non-native speakers. When encountering this, the test developers will have to decide whether the items with story problems should be excluded from the test, whether they can be improved, e.g., by using more frequently used words, or whether the test should be considered two-dimensional rather than one-dimensional \citep[see also][for a discussion of the connection between DIF and multidimensionality]{Ack:1992,Stretal:2021:APM}.

We will now illustrate how to use the R packages \texttt{glmertree} and \texttt{psychotree} to conduct the MOB analyses. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{\edc{Application Example 1}: Subgroup Detection in Mixed-Effects Models}
\label{sec:TutorialMixed}


<<data_prep_mixed, echo=FALSE>>=
## Load data
data <- data.frame(haven::read_dta("data_Deming_2008_0217_MF.dta"))

## Keep only eligible children in HS or no preschool
data <- data[data$Elig2_90 == 1, ]
data <- data[data$None2_90 == 1 | data$HS2_90 == 1, ]
data <- data[data$MotherID %in% names(table(data$MotherID) >= 2), ]
data$program <- factor(ifelse(data$None2_90 == 1, "None", 
                              ifelse(data$Pre2_90 == 1, "Pre", "HS")))

## Keep only families in which children differ in HS participation
selected_family <- rowSums(table(data$MotherID, data$program) > 0) == 2
selected_family <- names(selected_family)[selected_family]
data <- data[data$MotherID %in% selected_family, ]

## Node-specific predictor
data$program <- factor(ifelse(data$None2_90 == 1, "None", 
                              ifelse(data$Pre2_90 == 1, "Pre", "HS")),
                       levels = c("None", "HS"))

## Partitioning variables

## ageAFTQ: age-adjusted AFQT
## Race_child: Hispanic=Race_Child==1; Black=Race_Child==2; White=Race_Child==3
data$Race_Child <- factor(ifelse(data$Race_Child == 1, "Hispanic", 
                                 ifelse(data$Race_Child == 2, "Black", "White")))
## family income: PermInc
## Mom's years of education: MothED
part_vars <- c("AgeAFQT", "Race_Child", "PermInc", "Height_Mom81", "MothED")
id_vars <- c("ChildID", "MotherID")

## Outcome: Peabody Picture Vocabulary Test (PPVT)
testscore_vars <- c(paste0("PPVT_Raw", 2*(43:52)), paste0("AgeTest_Yr", 2*(43:52)))

## Create long dataset
pred_vars <- "program"
HS_dat <- reshape(data[ , c(part_vars, id_vars, testscore_vars, pred_vars)], 
                  varying = list(paste0("PPVT_Raw", 2*(43:52)),
                                 paste0("AgeTest_Yr", 2*(43:52))),
                  direction = "long", 
                  v.names = c("PPVT_Raw", "AgeTest_Yr"))
HS_dat <- HS_dat[ , -which(names(HS_dat) %in% c("time", "id"))]
names(HS_dat) <- c("AFTQ", "Race", "Income", "Mom_height", "Mom_edu_yrs", 
                   "ChildID", "MotherID", "Program", "PPVT", "Age")
HS_dat <- HS_dat[complete.cases(HS_dat), ]
HS_dat$Age <- sqrt(HS_dat$Age)
rownames(HS_dat) <- NULL
saveRDS(HS_dat, file = "HS_dat.Rda")
@

\subsection{Dataset}

To illustrate the use of GLMM trees, we analyze a dataset from \cite{Demi09}, who evaluated long-term benefits of participation in Head Start. Head Start is a federally funded nationwide \todo{what nation?} pre-school program for children from low-income families. \cite{Demi09} compared performance of siblings who differed in their participation in the program using data from the National Longitudinal Survey of Youth (NLSY; REF). 

The sample consists of \Sexpr{length(unique(data$MotherID))} families with at least two children, where at least one child participated in Head Start and at least one child did not participate in Head Start or any other preschool program. Data from children in those families who participated in another preschool program were excluded. As such, siblings who did not participate in Head Start serve as a natural control to assess the effects of participating in Head Start. As the outcome variable, we take the Peabody Picture Vocabulary Test (PPVT; REF) and model PPVT trajectories over time. As the timing metric, we took the square root of the child's age in years, which yielded a pattern of approximately linear increases over time.\todo{this would scare me a little as a reader: how would I know what to do/that I should do this? (Mirka: Maybe we can add that the sqrt or the log is a typical way to model trajectories that first increase rapidly, but then later increase more slowly?)}

Our dataset contains five family characteristics: The mother's score on the Armed Forced Qualification Test (AFTQ), adjusted for age; the families' income (averaged over the years for which data was available); race (Black, Hispanic or White); mother's years of completed education; mother's height. Note that the latter variable is one that should be completely irrelevant for predicting performance on a vocabulary test; it is included here to illustrate that the GLMM tree algorithm can fruitfully distinguish signal from noise variables. The dataset comprises data from families and children for whom complete data was available. 

We load the data and inspect the first rows:

<<>>=
HS_dat <- readRDS("HS_dat.Rda")
head(HS_dat, 3)
@

We inspect the complete dataset by plotting PPVT scores against age, separated by program participation: None (black) versus Head Start (red). To show the effect of age and Head Start participation, we fitted a mixed-effects model comprising their main and interaction effects. To account for the correlation between repeated assessments on the same child, we specified a random intercept. The results are presented in Figure~\ref{fig:global_lmm}, which shows that children participating in Head Start show slightly higher performance than their non-participating siblings and that this difference \edc{is slightly diminished but} persists over time. This result agrees with the findings of \cite{Demi09}.

\begin{figure}%
\caption{}
\begin{subfigure}{.7\textwidth}
<<echo=FALSE, fig=TRUE>>=
library("lme4")
lmm <- lmer(PPVT ~ Program*Age + (1|ChildID), data = HS_dat)
beta <- fixef(lmm)
plot(jitter(HS_dat$Age), HS_dat$PPVT, col = HS_dat$Program,
     cex = .5, #cex.lab = .7, cex.axis=.7, cex.main = .7,
     xlab = "Age (years)", ylab = "PPVT score", xaxt = "n",
     main = paste0("Global Model (", length(unique(HS_dat$ChildID)), " children)"))
abline(a = beta["(Intercept)"], b = beta["Age"], col = "black")
abline(a = beta["(Intercept)"]+beta["ProgramHS"], 
       b = beta["Age"]+beta["ProgramHS:Age"], col = "red")
axis(1, at=sqrt(c(3, 5, 8, 12, 17)), labels=c(3, 5, 8, 12, 17), cex.axis = .8)
@
\end{subfigure}
\label{fig:global_lmm}
\end{figure}%

\todo{code for figure not displayed on purpose? why only those particular x axis ticks?}

\FloatBarrier
\subsection{Linear mixed effects model tree}

\todo[inline]{I (Mirka) am confused: the nested structure in this example is timepoints (age) within children? Therefore, ChildID is the cluster-variable.  But then, there should be residual errors between the two children from the same family? In the text it says that this is accounted for by the random intercept, but I don't think this is the case (but maybe I am wrong?). Is it maybe accounted for by the cluster argument that is also said to ChildID? To me, the model specification is not yet clear from the text. Plus: If I am right, and the cluster is the child and the timepoints are on Level-1, I think all the covariates are on Level-2 (child level), and then maybe we should comment on the Type-I error problem?}
We now test whether the intercepts and slopes of the two regression lines differ as a function of the partitioning variables, using function \verb|lmertree| from R package \textbf{glmertree}:

<<>>=
library("glmertree")
HS_tree <- lmertree(PPVT ~ Program*Age | (1|ChildID) | AFTQ + Race + 
                      Income + Mom_edu_yrs + Mom_height, 
                    data = HS_dat, cluster = ChildID, minsize = 250)
@

With the first argument, we specified the model \verb|formula|, which has three parts separated by vertical bars: The left part (\verb|PPVT ~ Program*Age|) specifies the response variable, followed by a tilde (\verb|~|) and the fixed-effects predictors of relevance. The middle part (\texttt{1|ChildID}) specified the random effects\todo{(Mirka) Maybe add information that this is for each child, average DV across timepoints}. The right part (\verb|AFTQ + Race + Income + Mom_edu_yrs + Mom_height|) specified the partitioning variables: covariates that may possibly affect the values of the fixed-effects parameters. 

\todo[inline]{I (Mirka) am not sure what exactly the cluster argument does. Maybe Marjolein you have more information from your work with Achim. But my knowledge/intuition is that it has something to do with clustered covariances, so maybe it accounts for the sibling structure, but I don't think it accounts for the level of the predictor variables.}
With the second argument, we specified the dataset which contain the variables. Because we are dealing with repeated measurements on the same children, we additionally specified that the parameter stability tests should be performed on the child level using the \verb|cluster| argument. Using the default observation-level parameter stability tests may artificially inflate power\todo{Mirka: I'd rather say it has an inflated type-I error rate.}. Finally, because we want to retain large enough subgroups, we specified that the minimum number of observations in a terminal node should be 250.

Next, we plot the tree. With multiple fixed-effects predictors of interest, the default plots may become too crowded or difficult to interpret. We therefore specify \verb|type = "simple"| to facilitate interpretation, and using the \verb|nodesize_level| argument, we specified that the sample size printed above every terminal node should count the number of children, not the number of individual observations:

<<eval=FALSE>>=
plot(HS_tree, type = "simple", nodesize_level = 2)
@

\begin{figure}%
\caption{}
<<fig=TRUE, echo=FALSE, fig.width=9, fig.height=4>>=
plot(HS_tree, type = "simple", which = "tree", gp = gpar(cex = .5), nodesize_level = 2)
@

\vspace*{-3cm}

<<fig=TRUE, echo=FALSE, width=7.6, height=2.5>>=
nodes <- predict(HS_tree, type = "node")
beta <- coef(HS_tree)
par(mfrow = c(1, 4))
for (node in sort(unique(nodes))) {
  plot(HS_dat$Age[nodes == node], HS_dat$PPVT[nodes == node],
       col = HS_dat$Program[nodes == node],
       cex = .5, cex.lab = .7, cex.axis=.7, cex.main = .7,
       xlab = "Age", ylab = "PPVT", xlim = c(sqrt(3), sqrt(20)), ylim = c(0, 150))
  abline(a = beta[as.character(node), "(Intercept)"], b = beta[as.character(node), "Age"], col = "black")
  abline(a = beta[as.character(node), "(Intercept)"] + beta[as.character(node), "ProgramHS"], 
         b = beta[as.character(node), "Age"] + beta[as.character(node), "ProgramHS:Age"], col = "red")
}
@
\label{fig:lmm_tree}
\end{figure}%

The resulting tree is presented in Figure~\ref{fig:lmm_tree}. Below each terminal node, we plotted the observations and the two regression curves given by the coefficients in that node. The first split was made based on the AFTQ variable, which represents the mother's score on the Armed Forced Qualification Test, adjusted for the age at which they completed the test. The group with higher mother's AFTQ scores is further split based on race. The Black and Hispanic group is further split based on income. 

<<echo=FALSE, results=tex>>=
coefs <- round(coef(HS_tree), digits = 2)
design_df <- data.frame(.tree = rep(sort(unique(HS_tree$data$.tree)), each = 2),
                        Program = factor(rep(c("None", "HS"))))
design_df$`PPVT at age 6` <- predict(HS_tree$lmer, newdata = cbind(design_df, Age = sqrt(6)), 
                                     re.form = ~0)
design_df$`PPVT at age 18` <- predict(HS_tree$lmer, newdata = cbind(design_df, Age = sqrt(18)), 
                                      re.form = ~0)
names(design_df)[1] <- "Node"
knitr::kable(design_df, format = "latex", digits = 2, 
             caption = "Predicted PPVT scores at different ages.", 
             label = "predictions", centering = FALSE,
             linesep = "", # linesep command suppressess addlinesep every 5 rows
             row.names = FALSE, escape=TRUE, align = c("cccc"), booktabs = TRUE)
@

To aid interpretation of the coefficients in Figure~\ref{fig:lmm_tree}, Table~\ref{tab:predictions} provides predicted PPVT scores for each of the groups and programs at ages 6 \todo{x axis for age starts at 2?} and 18. All nodes show a modest benefit of Head Start participation at age 6, about 3-4 points on the PPVT. This benefit remains the same over time for the group with lower mother's AFTQ scores. The benefit increases over time for White children with higher mother's AFTQ scores. Strikingly,\todo{possibly less extreme word like: Interestingly,} the benefit decreases over time for Black and Hispanic children with higher mother's AFTQ scores. These results correspond to the conclusions of \cite{Demi09}. \todo[inline]{Generally the effects are not large but the example is very clear and I like that it is an intervention effect. I suggest to state early before the figure that the effects are not large in this example but we are interested in the group differences; also try to make figure bigger and wider to make slopes more visible; (Mirka): I think the main effect of the intervention and the interaction effect is not too small, but it is hard to see because the age effect is so pronounced in this example. Would it be an option to only use the intervention effect in the end nodes? }

To evaluate whether the detected subgroups indeed contribute to better predictions, we used cross validation. We \edc{randomly} separated the \Sexpr{length(unique(HS_dat$MotherID))} families in the dataset into ten equally-sized folds. We took nine of the ten folds as a training dataset on which we fitted two models: an LMM tree \edc{like the one shown above} (i.e., an LMM with subgroups \todo{necessary? confused me}) and an LMM without subgroups (comprising main and interaction effects of age and Head Start participation, and a random intercept with respect to child). We evaluated performance on the remaining folds by computing and evaluating accuracy of the predictions. \todo{explain MSE and $R^2$} We repeated this procedure ten times, so that all folds were used as a test set once. The results are presented in Table~\ref{tab:performance}, which shows that LMM trees generalize well: They provide better predictive accuracy compared to LMMs, while implementing only few splits. 





<<cv_lmm_tree, echo=FALSE, results=tex>>=
results <- data.frame(MSE_tree = rep(NA, times = 10), 
                      MSE_lmm = rep(NA, times = 10),
                      tree_size = rep(NA, times = 10))

set.seed(42)
folds <- sample(1:10, replace = TRUE, size = length(unique(HS_dat$MotherID)))
HS_dat$fold <- NA
for (mom in unique(HS_dat$MotherID)) {
  HS_dat$fold[HS_dat$MotherID == mom] <- folds[unique(HS_dat$MotherID) == mom]
}
for (i in 1:10) {
  ## Fit models on training data
  lmmt <- lmertree(PPVT ~ Age*Program | ChildID | AFTQ + Race + Income + 
                     Mom_height + Mom_edu_yrs, 
                   cluster = ChildID, data = HS_dat[HS_dat$fold != i, ], 
                   minsize = 225)
  lmm <- lmer(PPVT ~ Age*Program + (1|ChildID), 
              data = HS_dat[HS_dat$fold != i, ])
  
  ## Evaluate performance on test data
  results$tree_size[i] <- (length(lmmt$tree)-1)/2
  preds <- predict(lmmt, newdata = HS_dat[HS_dat$fold == i, ], re.form = ~0)
  results$MSE_tree[i] <- mean((preds - HS_dat[HS_dat$fold == i, "PPVT"])^2)
  preds <- predict(lmm, newdata = HS_dat[HS_dat$fold == i, ], re.form = ~0)
  results$MSE_lmm[i] <- mean((preds - HS_dat[HS_dat$fold == i, "PPVT"])^2)
}
res_df <- data.frame(method = c("LMM tree", "LMM"),
                     MSE = colMeans(results[1:2]),
                     SD = sapply(results, sd)[1:2],
                     `number of splits` = c(colMeans(results)[3], NA),
                     R2 = colMeans(1 - results[ , 1:2] / var(HS_dat$PPVT)))
knitr::kable(res_df, format = "latex", digits = 3, 
             caption = "Cross-validated performance of LMMs and LMM trees.", 
             label = "performance", centering = FALSE,
             linesep = "", # linesep command suppressess addlinesep every 5 rows
             row.names = FALSE, escape=TRUE, align = c("ccccc"), booktabs = TRUE)
@



\todo[inline]{I am not sure this dataset is the best GLMM tree illustration, because visualized differences between subgroups are very small and perhaps the fixed-effect part is already too complex. Alternative: Do not focus on effect of Head Start, but use as partitioning variable. Then can also use data from more families (HS or none not needed), partition using child-level characteristics, and  model specification becomes simpler. -- response Caro: I like it, see comment above: try to prime that effects are not large -- response Mirka: I think that's okay. Only if you had data at hand that is not longitudinal (rather students in classes/schools), with covariates on Level-1, I would opt for that because it doesn't have the Level-2 splitting issue. }



\newpage
\FloatBarrier
\section{\edc{Application Example 2}: Subgroup Detection in Rasch Models}
\label{sec:TutorialRasch}

\begin{itemize}
\item Example Rasch tree: Data from SPISA (data from Trepte \& Verbeet)
\item take the subsample of students (only students), and a random subsample of 5000 respondents (seed 04102023)
\item items could probably appear in Appendix??
\begin{itemize}
\item Which painter created this painting? – Andy Warhol.
\item What do these four buildings have in common? – All four were designed by the same architects.
\item Roman numbers: What is the meaning of CLVI? – 156.
\item What was the German movie with the most viewers since 1990? – Der Schuh des Manitu.
\item In which TV series was the US president portrayed by an African American actor for a long time? – 24.
\item What is the name of the bestselling novel by Daniel Kehlmann? – Die Vermessung der Welt (Measuring The World).
\item Which city is the setting for the novel ‘Buddenbrooks’? – Lübeck.
\item In which city is this building located? – Paris.
\item Which one of the following operas is not by Mozart? – Aida.
\end{itemize}
\end{itemize}

\todo[inline]{at Caro: Ich habe die Daten (glaube ich) mal von dir bekommen (\texttt{spisa\_ges.RData}) und die Item-Formulierungen habe ich aktuell aus dem psychotree Paket. Stimmt das so? Ich glaube, der Originaltest hatte mehrere parallele Skalen, zumindest sind in dem PDF/BUCH von Trepte sehr viel mehr, und sehr ähnliche Items enthalten. Das könnten wir vielleicht nochmal nachgucken.}

We load the data and inspect the first rows. there are several covariates: age (continuous), gender (male, female, missing), area (Language \& Culture, Law \& Economics, Medicine \& Health, Engineering, Sciences, Pharmacy, Geography, Agriculture \& Nutrition, Sports) and 9 items from the culture scale (true/false 0/1). 

@Caro: I excluded the covariates student and occupation, because I took the student subsample; 

<<echo = FALSE>>=
load("SPISA_example/dat_SPISA.rda")
covar_data$Student <- NULL
covar_data$Occupation <- NULL
@


<<>>=
head(covar_data)
head(culture_scale)
@

We create a dataset \texttt{dat\_SPISA} that contains the data of the covariate and the item responses from the culture scale. By assigning the whole scale to the dataset using \texttt{\$}, we can later access item responses from all items using \texttt{\$culture}. 
<<>>=
dat_SPISA <- covar_data
dat_SPISA$culture <- culture_scale
@

The Rasch tree can be fitted with the function \texttt{raschtree} from the R package \texttt{psychotree}. We use the typical formula syntax with the item responses of the culture scale on the left hand side of $\sim$, and the covariates on the right hand side. 

<<echo = FALSE>>=
if(!file.exists("SPISA_example/Raschtree.rda")){
  library(psychotree)
  Raschtree_culture <- raschtree(culture ~  Gender + Age + Area, 
                                 data = dat_SPISA)
  save(Raschtree_culture, file = "SPISA_example/Raschtree.rda")
}
load("SPISA_example/Raschtree.rda")
@


<<eval = FALSE>>=
library(psychotree)
Raschtree_culture <- raschtree(culture ~  Gender + Age + Area,
                               data = dat_SPISA)
@

We can plot the Raschtree using the plot function. But it is very big, lots of splits, we don't know whether the splits are due to substantial differences in item difficulty parameters, because the item difficulty profiles in the end nodes are hard to interpret/compare. 

<<fig = TRUE, height = 8, width = 15>>=
plot(Raschtree_culture)
@

Mantel-Haenszel trees use the Mantel-Haenszel effect size measure for DIF. it has three categories (A: negligible, B: moderate, C: large). If none of the items has DIF in category B or C (default, but can also be changed so that DIF has to be C), the tree is stopped from growing. 

The software must be installed from github, but will (probably) be implemented in the psychotree package some time in the future

<<>>=
devtools::install_github("mirka-henninger/raschtreeMH")
library(raschtreeMH)
@

Syntax is very similar to the classical Rasch tree, but an additional argument \texttt{stopfun}, where the mantelhaneszel stopping function can be selected (but also own stopping functions can be provided; see Henninger et al., 2023). The user also has to indicate what kind of purification strategy should be used (none, 2step, iterative). Iterative is recommended. 

<<echo = FALSE>>=
if(!file.exists("SPISA_example/Raschtree.rda")){
  library(psychotree)
  Raschtree_MH_culture <- raschtree(culture ~  Gender + Age + Area, 
                                  data = dat_SPISA, 
                                  stopfun= stopfun_mantelhaenszel(purification = "iterative"))
  Raschtree_MH_culture <- add_mantelhaenszel(Raschtree_MH_culture, purification = "iterative")
  save(Raschtree_MH_culture, file = "SPISA_example/Raschtree_MH_culture.rda")
}
load("SPISA_example/Raschtree_MH_culture.rda")
@

<<eval = FALSE>>=
Raschtree_MH_culture <- raschtree(culture ~  Gender + Age + Area, 
                                  data = dat_SPISA, 
                                  stopfun= stopfun_mantelhaenszel(
                                    purification = "iterative"))
@

For technical reasons, the information about the effect size and ETS classification are not saved in the tree object itself but have to be added afterwards using the \texttt{add\_mantelhaenszel} function. The information can then be accessed from the object using \texttt{\$info\$mantelhaenszel}

<<eval = FALSE>>=
Raschtree_MH_culture <- add_mantelhaenszel(Raschtree_MH_culture, 
                                           purification = "iterative")
Raschtree_MH_culture$info$mantelhaenszel
@

\todo[inline]{not clear what this information is necessary/useful for? since it is not displayed, leave this out?}

If we now plot the Mantel-Haenszel Rasch tree, we see that the tree is much more concise with a lower number of splits. In addition, we see the number of items classified as A, B, or C in each node. For instance, we see that one item is classified as B in Node 1, and no item as C.

<<fig = TRUE, width = 12, height = 7>>=
plot(Raschtree_MH_culture)
@

We can also color the items in the end node profiles according to a split in the tree. Here, we see that for Node 1 (split on the covariate Area), item 2 shows DIF in category B. Item 2 seems to be easier (y-axis shows item easiness) in certain areas compared to Language, Culture, Medicine, Health, Arts. 

<<fig = TRUE, width = 12, height = 7>>=
plot(Raschtree_MH_culture, color_by_node = 1)
@

We can also color by Node 3 (also split on covariate Area). Also here, Item 2 shows DIF. Similarly, item 2 seems to be more difficult (less easy) for respondents who study Arts. 

<<fig = TRUE, width = 12, height = 7>>=
plot(Raschtree_MH_culture, color_by_node = 3)
@

This is interesting, because it is the culture scale. Why should people who study Arts have more problems to solve the item? When we look at the item, it probably makes sense: the items asks what four buildings have in common (Allianz-Arena München, Tate Modern NY, Olympia-Stadion Peking, Elbphilharmonie Hamburg). Two of these buildings are sports stadiums. So probably respondents who are interested in sports, and know these buildings well have an advantage on the item (independent of the level of knowledge about culture). 

\todo[inline]{Maybe a small note on anchoring? Item difficulty profiles are anchoring on the items that do not show DIF. This facilitates comparisons across end nodes (but maybe we do not want to say that??)}

\FloatBarrier
\section{Discussion}

In this tutorial paper we have outlined the rationale of the MOB algorithm and how it can be used for detecting parameter differences in mixed-effects and Rasch models. The main advantage of MOB is that it can detect groups of persons with different model parameters in a data driven way. This makes it more flexible for detecting differences that were not hypothesized by the researchers. For example in DIF analysis, it is often the case that any obvious sources of DIF have already been avoided by the content expernts in the item creation phase. Any remaining DIF is unexpected. Therefore, any DIF detection approach that relies on the researchers correctly specifying the exact groups of persons that exhibit DIF can miss DIF if it is associated with other (combinations of) covariates or other cutpoints than the ones investigated. In this sense, a more exploratory approach like a Rasch tree has a higher statistical power to detect DIF in previously unknown groups \citep{StrKopZei:2015:P}. The same holds for parameter differences in mixed-effects models, where the substantial hypothesis are typically about the fixed effects of the interventions, not necessarily about all possible subgroup-specific .... \todo{Marjolein add if this makes any sense here}

\todo[inline]{Write summary of findings.}

For our two application examples, we could show that MOB can detect ..., performs automated variable selection \todo[inline]{or did you, Marjolein, mean repeat substantive findings? not very exciting for SPISA} 

\todo[inline]{Mention shortcomings.}

not possible to detect certain patterns: XOR problem, known for all trees

stability of trees, small changes in data can lead to quite different looking tree. The tree structure should always be interpreted jointly for all variables: rather than considering the first splitting variable to be the most important one, it is actually the combination of all splitting variables that creates the groups.\todo{in interpretation of trees in examples: do this, point out that it is interactions -- Caro also add in intro?}. The \texttt{stabletree} R package also provides descriptive statistics and graphics that help judge the stability of tree and MOB results \citep{PhiRusHor:2018:JoCaGS,PhiZeiStr:2016}. 


\todo[inline]{Write about future work.}

sample size dependence, similar ideas like MH trees also possible for other MOB?

The Mantel-Haenszel effect size measure for DIF will be integrated in the \texttt{psychotools} package. 



\bibliography{_MOB_paper}

\todo{Add DOIs.}

\newpage
\appendix



\section{Appendix A: SDQ-II items administered in the ECLS}
\label{sec:AppendixA}

\todo{Fix formatting.}

How true is each of these about you? (1="not at all true"; 2="a little bit true"; 3="mostly true" or 4="very true")

\begin{itemize}

\item C7MTHBST (math): Math is one of my best subjects.

\item C7ANGRY (internalizing): I feel angry when I have trouble learning.

\item C7LIKRD (reading): I like reading.

\item C7WRYTST (internalizing): I worry about taking tests.

\item C7MTHGD (math): I get good grades in math.

\item C7LONLY (internalizing): I often feel lonely.

\item C7ENGBST (reading): English is one of my best subjects.

\item C7SAD (internalizing): I feel sad a lot of the time.

\item C7LIKMTH (math): I like math.

\item C7WRYWEL (internalizing): I worry about doing well in school.

\item C7ENJRD (reading): I enjoy doing work in reading.

\item C7WRYFIN (internalizing): I worry about finishing my work.

\item C7ENJMTH (math): I enjoy doing work in math.

\item C7WRYHNG (internalizing): I worry about having someone to hang out with at school.

\item C7GRDENG (reading): I get good grades in English.

\item C7ASHAME (internalizing): I feel ashamed when I make mistakes at school.

\end{itemize}


\end{document}