\documentclass[doc,floatsintext,natbib]{apa7}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{doi}
\usepackage{booktabs}
%\usepackage[dvipsnames]{xcolor}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{setspace} % for adjusting line spread
\usepackage{hyperref} 

\renewcommand{\doitext}{} % prevents superfluous addition of "doi:" in references
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % uses symbols instead of numbers for footnotes
\usepackage[colorinlistoftodos, textsize=footnotesize]{todonotes}
%%\todo[inline, color=green!40]{This is a green inline comment.}

%text from Marjolein edited by Caro
\newcommand{\edc}[1]{\textcolor{blue}{#1}}

\linespread{1.3}


\title{One Model May Not Fit All: Subgroup Detection Using Model-Based Recursive Partitioning}
\shorttitle{shorttitle}

\authorsnames{Marjolein Fokkema$^1$, Mirka Henninger$^2$ \& Carolin Strobl$^2$}
\authorsaffiliations{$^1$Leiden University, $^2$Universit\"at Z\"urich}

\abstract{Model-based recursive partitioning \citep[MOB,][]{ZeilyHoth08} is a flexible framework for detecting subgroups of persons showing different effects in a wide range of parametric models. It provides a versatile tool for detecting and explaining heterogeneity of intervention effects. In this tutorial paper, we provide an introduction to the general MOB framework. In two specific case studies, we show how MOB-based methods can be used to detect and explain heterogeneity in two widely-used frameworks in educational studies: mixed-effects and item response theory (IRT) models. In the first case study, we show how GLMM trees \citep{FokkySmit18} can be used to detect subgroups with different parameters in mixed-effects models. We apply GLMM trees to longitudinal data from a study on the effects of Head Start, to identify subgroups of families where children show comparatively larger or smaller gains in performance. In a second case study, we show how Rasch trees \citep{StroyKopf15} can be used to detect subgroups with different item parameters in IRT models, i.e.~differential item functioning (DIF). %DIF should be investigated before using test results for comparing groups, because undetected DIF can affect test fairness. % Test fairness is more related to use of test scores in practice than group-level comparisons I would think?
We show how a recently developed stopping criterion \citep{HenDebStr:2023:EPM} can be used to guide subgroup detection based on DIF effect sizes.\\}


\begin{document}
\SweaveOpts{concordance=TRUE}
\renewenvironment{Schunk}{\small}{}

<<echo=FALSE>>=
## suppress printing of > and + before code, it is annoying for copy-pasting
options(prompt=" ", continue = " ") 
@

\maketitle

\todo[inline]{Target journal: \textit{Journal of School Psychology}. This Special Issue is on "Conceptual and Methodological Advances for Understanding Contextual, Identity, and Cultural Effects in Intervention Research". Submission deadline: October 31, 2023}


\newpage
\section{Introduction}
\label{sec:Introduction}

Model-based recursive partitioning \citep[MOB;][]{ZeilyHoth08} is a semi-parametric approach for detecting differences in the parameters of a statistical model between groups of persons. It generalizes the principle of recursive partitioning, that is also used in classification and regression trees \citep[CART; ][]{Breetal:1984}. 
In CART, the aim is to detect groups of persons, defined by (combinations of) covariate values, that differ in their mean on a response variable. For example, if we want to predict math exam grades using student characteristics assessed in a previous year, we could find that the subgroup of children who are highly motivated and have good reading skills show higher average math exam grades. 
MOB generalizes this idea: It also identifies groups of persons defined by (combinations of) covariate values, but the groups can differ in a wider range of parameters of a statistical model, instead of only the mean. An example could be that children who are highly motivated and have good reading skills show higher slopes in a regression model relating the time spent studying to math exam grades. 

The framework of MOB is very flexible and it can be applied to a wide range of statistical models, such as linear and generalized linear regression \citep{KopAugStr:2013,ZeilyHoth08} or models for paired-comparison data \citep{StrWicZei:2011:JoEaBS,WiedyFrick21}. In this article, we will highlight two specific MOB methods that we consider particularly relevant for research in school psychology: MOB for mixed-effects models \citep{FokkySmit18} and MOB for Rasch measurement and Item Response Theory models \citep[IRT,][]{StrKopZei:2015:P,KomStrZei:2017:EaPM,HenDebStr:2023:EPM}. Mixed-effects models become relevant whenever data are collected in repeated measures or nested data structures, for example when children are tested at several time points (so that time points are nested in children) and/or when children from different classes from different schools (so that children are nested in classes, which are in turn nested in schools) participate in a study. In application example 1 \ref{sec:TutorialMixed} we will illustrate how MOB can be used to detect subgroup-specific intervention effects while taking into account the nested data structure. In this example, the focus is on detecting subgroups with different parameters of a \textit{regression} model. We thus assume that the psychological test score(s) of interest have already been validated. 
%\todo{(Caro: I have kept psychological test score, because instrument to me sounds more clinical)(Mirka: I like the term instrument to distinguish it from a statistical test) (Marjolein: I hope 'psychological test score' clears up possible confusion)} 


For application example 2, we go back in the research process to the point where a new psychological test has been administered to a validation sample. Here, we focus on detecting subgroups with different parameters of a \textit{measurement} model. Validity assessment requires that we make sure that test results are comparable, for example between children of different genders. If certain items show different measurement parameters between groups, this may put certain groups at a relative disadvantage, and these items are said to violate measurement invariance or to exhibit differential item functioning (DIF). In order to test for DIF in the framework of Item Response Theory or Rasch modelling \citep{AnthyDiPe16,DebStrZei:2022:CRC,Mall97}, the item parameters are compared between groups of persons. This can be done in a way that allows to detect DIF, while accounting for possible true group differences in ability. Traditional approaches for testing DIF require the groups to be pre-specified in order to test for DIF. In application example 2 we show how MOB flexibly allows to detect groups with different item parameters in a data-driven way. 

In the following section, we first give a short introduction into the algorithm and statistical concepts behind MOB. Readers interested in learning more about its predecessor method, classification and regression trees, are referred to the introduction by \citet{StrMalTut:2009:PM}. 

\section{The MOB Algorithm}

The main rationale of MOB is that one global model may not fit all observations in a dataset equally well. In many studies, additional covariates may be available. It may then be possible to uncover subgroups defined by these covariates, and obtain better-fitting models in each of those subgroups \citep{ZeilyHoth08}. 

<<eval = TRUE, echo = FALSE>>=
set.seed(3)
x1 <- runif(250, min = 5, max = 11)
x2 <- runif(250, min = 2, max = 8)
x3 <- runif(250, min = 4, max = 10)
error <- rnorm(250, sd = 1.5)
y <- 7 + ifelse(x1 > 8 & x2 > 5, yes = 2.5, no = -2.5) + error
toy_data <- round(data.frame(x1, x2, x3, y), digits = 2L)
toy_data$x2 <- factor(ifelse(x2 > 5, yes = ifelse(x2 > 6.5, "A", "C"), no = "B"))
@

We illustrate the idea using a very simple, simulated toy dataset, comprising \Sexpr{nrow(toy_data)} observations and four variables: A continuous response variable $y$, and three covariates, $x_1$, $x_2$ and $x_3$, as possible partitioning variables. 
%note from Caro: I woul call them partitioning variables or covariates, but not predictors, to save the term predictors for those x-variables that are part of the parametric model
To keep the example simple, we apply MOB to a very simple global model, which comprises only an intercept. It would also be possible to use, e.g., a linear regression model or a logistic regression model as the global model. Figure~\ref{fig:toy}, left, shows the distribution of $y$ in a boxplot, with the global intercept indicated as a triangle. Obviously, the global intercept does not describe all observations equally well, there is quite some unexplained variation around it.
%note from Caro: I found Fig 1 containing only the boxplot irritating and have moved it into one panel with the tree

To detect possible subgroups with different values for the parameters, MOB cycles iteratively through the following steps:

\begin{enumerate}
\setlength\itemsep{0.25em}
\item The model parameters are first estimated jointly for all persons in the current node, starting with the root node containing the full sample.
\item Structural change in the model parameters is assessed with respect to each available covariate.
\item If there is significant structural change, the observations in the current node are split using the covariate associated with the strongest change.
\item Steps 1--3 are repeated recursively in each resulting node until there is no more significant structural change (or the groups becomes too small).
\end{enumerate}

We applied MOB to the observations in Figure~\ref{fig:toy}, left, specifying $x_1$, $x_2$ and $x_3$ as potential splitting variables. The resulting tree is shown in Figure~\ref{fig:toy}, right. In the root node, which contains all observations (step 1), structural change tests were performed for each of the three covariates (step 2). The tests revealed that $x_1$, a continuous covariate, was most strongly associated with instabilities in the intercept and $x_1$ was thus selected first for splitting (step 3). The $p$ value resulting from the structural change test for the first split in $x_1$ is depicted in the root node. Next, the cutpoint for $x_1$ was selected so that the two resulting subgroups exhibit the strongest parameter differences. In the right daughter node, additional significant instability was detected with respect to $x_2$, a categorical covariate. Again, the cutpoint in $x_2$ was selected so that the two resulting subgroups exhibit strongest parameter differences. In the left daughter node (node 2), no further splits were created, because none of the three covariates were significantly associated with any further instability in this subgroup (step 4). The same held for nodes 4 and 5, so that the third covariate, $x_3$, was never selected for splitting. The subgroup-specific distributions of the response variable are presented in end nodes at the bottom of the tree.
The fact that Figure~\ref{fig:toy}, right, displays more than one end node confirms that one global model for all observations cannot appropriately capture the pattern in the data. 
Figure~\ref{fig:toy}, right, also shows that, out of the three covariates that were presented to the algorithm, only two were actually selected for splitting. This automatic variable selection is an important characteristic of classification and regression tree and MOB algorithms. 

\begin{figure}
\caption{Left: Univariate distribution of the response variable, with global mean indicated by triangle. Right: Tree with group-specific distributions of the response variable in the terminal nodes.}
\begin{subfigure}[][][t]{.4\textwidth}
<<eval = TRUE, echo = FALSE, fig=TRUE, width=4.5, height=7>>=
library("colorspace")
boxplot(toy_data$y, cex = .7)
points(x = 1, y = mean(toy_data$y), col = rainbow_hcl(1), pch = 17, ylim = c(1,14))
@
\end{subfigure}
\begin{subfigure}[][][b]{.6\textwidth}
<<eval = TRUE, echo = FALSE, fig=TRUE, width=6, height=5>>=
library("partykit")
tree <- lmtree(y ~ 1 | x1 + x2 + x3, data = toy_data)
plot(tree, gp = gpar(cex = .7), ylim = c(1,14))
@
<<eval = FALSE, echo = FALSE, fig=TRUE, width=5, height=5>>=
plot(tree, gp = gpar(cex = .7), type = "simple")
@
\end{subfigure}
\label{fig:toy}
\end{figure}


%The subgroups are defined by the covariates that are used for splitting and together with cutpoints in those covariates. For example, in Figure~\ref{fig:lmm_tree} for the metric covariates \texttt{AFTQ} and \texttt{Income} a numeric cutpoint has been selected by the MOB algorithm to separate the groups. The algorithm has identified this cutpoint as being the location of the strongest parameter difference between the two resulting groups. The categorical covariate \texttt{Race}, which was coded in three categories in this example, has been divided into two groups of categories: Black and Hispanic vs.~White. For a binary covariate, \todo{like ... -- add name if is there one in the other example?} 
%on the other hand, there is only one possible cutpoint, namely between the two categories. 

%While in Figure~\ref{fig:lmm_tree} each end node contains a linear mixed-effecs model with a group-specific effect for the Head Start intervention, in Figures~\ref{fig:math_tree} and \ref{fig:read_tree} each end node contains the group-specific item parameters of a Rasch model for the respective test items. 
%The fact that the Figures display more than one end node already means that one joint mixed-effects or Rasch model was not appropriate to describe the pattern in the data. 

%Figure~\ref{fig:lmm_tree} also shows that, out of the  covariates that were presented to the algorithm (\texttt{AFTQ}, \texttt{Race}, \texttt{Income},\texttt{Mom\_edu\_yrs}, and \texttt{Mom\_height}), only \texttt{AFTQ}, \texttt{Race}, and \texttt{Income} were actually selected for splitting. Variables are selected based on a statistical test for parameter differences, also termed a test for structural change. Structural change is present if, for example, the model parameters systematically differ for children from lower vs.~higher income families. At each node, the algorithm will select the covariate showing the strongest structural change as the next splitting variable. Within this variables, the optimal cutpoint is chosen in a separate step.

%While it is also possible to create more than two groups in each split \citep{KimLoh:2001,Qui:1993}, binary splitting algorithms are typically preferred because they lead to more concise trees. % Think we can leave this sentence out, I'm not convinced readers will consider that other than binary splits may be implemented.


%note from Caro: I did not like the formulation "There are two main differences between MOB and and traditional classification and regression tree algorithms" because readers might misinterpret it and think that only MOB, not trees, can be unbiased and use significance tests, while the distinction is really between traditional and modern algorithms for both trees and mob, changed into: characteristics
There are three further important characteristics of tree and MOB algorithms that we would like to mention here: The first characteristic is the type of variable and cutpoint selection an algorithm employs. Traditional classification and regression tree algorithms, like those of \citet{Breetal:1984} and \citet{Qui:1993}, performed variable and cutpoint selection in one step, which leads to an undesirable behavior called variable selection bias. That is, the traditional algorithms prefer variables offering more possible cutpoints in the selection process -- regardless of their true information content. An algorithm that still has this problem is, for example, the \texttt{rpart} algorithm in R, based on the original CART algorithm by \citet{Breetal:1984}. More modern algorithms for classification and regression trees have solved this problem and offer unbiased variable selection, such as \texttt{QUEST} \citep{LohShi:1997}, which is available in SPSS, and \texttt{ctree} \citep{Hotetal:2006}, which is available in \texttt{R} in packages \texttt{party} and \texttt{partykit} \citep{partykit:pkg}. The latter forms the basis for all MOB approaches presented in this paper. For more details on the statistical theory behind unbiased classification and regression trees and MOB, see \citet{Hotetal:2006,StrMalTut:2009:PM,StroyKopf15}.  

The second characteristic relates to the way a tree or MOB algorithm stops splitting: Modern algorithms for classification and regression trees and MOB use a criterion of statistical significance to stop splitting. Once there are no more covariates that show a significant structural change in any node, splitting is halted. In this way, the  algorithm selects only those partitioning variables that are relevant for distinguishing the groups (i.e., it performs automatic variable selection, as illustrated in Figure~\ref{fig:toy}, right). Moreover, the trees will not grow as large as possible, but will stop when no more significant structural change is detected. While traditional tree algorithms like those of \citet{Breetal:1984} and \citet{Qui:1993} grew very large trees and then cut them back (so called \textit{pruning}), modern tree and MOB algorithms employ significance tests as stopping criteria (and some can also use effect size measures, as we will see in application example 2). This allows for stopping tree growing as soon as no significant differences can be detected anymore. Other stopping criteria are based on the number of persons in the end nodes. These criteria ensure that the sample sizes in the end nodes are large enough to estimate the statistical model in each end node.\todo{we should discuss this in the application examples; there are some defaults but for models with many parameters it might make sense to increase them}

The third important characteristic of classification and regression trees as well as MOB is that the entire structure identified by the trees does not have to be pre-specified by the researcher in a confirmatory manner, but is learned from the data in an exploratory manner. This is a key feature of the MOB approach that makes it very flexible and sets it apart from purely parametric approaches, where only those main effects and interactions that are explicitly included in the specification of the model are considered. While there are phases in psychological and educational research where it is very important to specify hypotheses a-priori and test them in a confirmatory manner, in early stages of research exploratory methods are an important addition to the statistical toolbox for researchers. Still, an important challenge for the researcher remains: To specify the parametric model of interest, to specify the set of possible partitioning variables and to choose the settings of the MOB algorithm. The current paper aims to provide some guidance. 
%note from Caro: it said include link in several places when referring to example 2, but I don't think this is necessary, because currently sections are not numbered anyway and the title example 2 is clear

The general framework of MOB introduced above will now be applied to two types of statistical models that are particularly relevant in school psychology research: Mixed-effects models for repeated measures or nested data structures and measurement models for validating psychological and educational tests. 

\subsection{Using MOB for Subgroup Detection in Mixed-Effects Models}

Mixed-effects models contain two types of effects: Fixed and random effects. Fixed effects are typically used to capture population-averaged effects, while random effects are used to capture inter-individual variation deviating from these fixed effects. In many studies, researchers are specifically interested in testing hypotheses relating to the population-averaged effects, while the random effects are included in the model to properly account for inter-individual variation, and correlations between observations within the same unit.

GLMM trees combine MOB and mixed-effects models and were introduced by \cite{FokkySmit18}. Because researchers' interests commonly focus on the fixed effects (of time or treatment, for example), the GLMM tree algorithm only targets structural change in the fixed-effects parameters. The random-effects parameters can be specified as usual and are assumed constant; they are estimated using all observations in the dataset. The MOB algorithm is thus applied to the fixed-effects parameters only and the detected subgroups will only obtain different estimates for the fixed-effects coefficients. This allows for detecting subgroups with different means on the response variable, but also with differential treatment effects or differential growth over time, to name but a few examples. While the 'standard' MOB trees \cite{ZeilyHoth08} allow for partitioning fixed-effects GLMs, GLMM trees additionally estimate and account for random effects and can thus be used for partitioning mixed-effects regression models. Further mathematical and computational details about the GLMM tree algorithm are described in \cite{FokkySmit18} and \citep{FokkyZeil23}. 



\subsection{Using MOB for Detecting DIF in Measurement Models}

Validity assessment of scores on psychological or educational tests requires researchers to assess whether the same construct is measured in the same way for different groups. In the framework of IRT and Rasch measurement, test items are typically investigated with respect to item misfit, multidimensionality and other violations of the measurement model \citep[cf., e.g.,][for an introduction]{DebStrZei:2022:CRC}. \todo{Omit last sentence or move to discussion? I think better to focus on DIF only here. Response Caro: would rather keep it here, but have increased importance of measurement invariance in next sentence.} A particularly crucial assumption for the comparability of test scores between groups is measurement invariance. Items that violate measurement invariance by showing different measurement properties for different groups of participants display DIF.

MOB can be used to detect DIF by means of testing whether the item parameters of the measurement model exhibit significant instability with respect to (combinations of) covariates. For the Rasch measurement model, the R function for conducting the MOB analysis is the \texttt{raschtree} function from R package \texttt{psychotree} \citep{StroyKopf15}. The usage of this function will be illustrated in application example 2. We will see that, just like for  GLMM trees, we need to specify which variables are part of the parametric model. In the case of the Rasch model, this will be the test items. Moreover, we need to specify, which covariates are made available to the MOB algorithm for selecting relevant splitting variables and cutpoints. 

If one joint Rasch model holds for the entire sample, i.e., if there is no DIF, a Rasch tree should show no splits. However, in certain settings in educational research, such as large scale assessments, very large sample sizes are available for testing for DIF. Large sample sizes are good for detecting even small effects or model violations with a high statistical power. The same holds for the statistical tests used for detecting parameter change in the MOB algorithm, so that in larger samples even very small parameter differences can be detected with large samples. However, in DIF detection this may mean that even very small DIF effects, that in practice can be considered ignorable, will be detected if only the sample is big enough. As we will illustrate in the first part of application example 2, for a large sample of university students who have taken a quiz to test their general knowledge, this can lead to very large trees, that are hard to interpret and contain splits that would not be considered relevant by measurement experts. Therefore, an extension of Rasch trees has been suggested by \citet{HenDebStr:2023:EPM} based on the Mantel-Haenszel effect size measure for DIF. \citet{HolTha:1985} have suggested an intuitive classification of DIF effect sizes based in the Mantel-Haenszel statistic, that is being widely used in educational testing. In this classification, category A stands for negligible DIF (small effect size or not statistically significant), B for medium DIF (neither A nor C), and C for large DIF (large effect size and statistically significant). \citet{HenDebStr:2023:EPM} have incorporated this classification as an additional stopping criterion for Rasch trees, so that the user can decide, for example, that a split should only be conducted if the detected DIF is of category B or C, while negligible DIF of category A should be ignored. 

As we will show in application example 2, for large sample sizes this can be very helpful because it results in shorter trees that are easier to interpret and contain only splits corresponding to DIF effect sizes considered relevant in practice. Together with a purification step and anchoring \citep[see][and application example 2 for details]{HenDebStr:2023:EPM}, the Mantel-Haenszel classification can also be used for highlighting those items that show DIF with respect to certain groups of persons graphically. This can help generate hypothesis about the sources of DIF, as we will ilustrate in application example 2, and can also aid the decision how to proceed with the DIF items. 
%\todo[inline]{following text from Caro can be used here or later after example 2 or in discussion}
For example, items that show DIF between different language groups can often be improved by means of making sure that in all translations the meaning is as similar as possible, that the words employed in the translations for the different languages are equally frequently used, etc. In other situations, sources of DIF might be harder to eliminate, so that often DIF items are excluded from a test. Either way, the measurement model needs to be refitted and its assumptions checked again after the final set of items has been decided upon and, in the case of modified items, administered again. 

Another important aspect to keep in mind is that DIF can be caused by one or more items measuring a secondary dimension in addition to the dimension that is intended to be measured by the test. This would be the case in instruments intended to measure math aptitude containing pure algebra problems as well as story problems. Students whose native language is not the same as the test language can have a disadvantage in answering the story problems, for example when they contain seldomly used words. These items will then show DIF between native and non-native speakers. When encountering this, the test developers will have to decide whether the items with story problems should be excluded from the test, whether they can be improved, e.g., by using more frequently used words, or whether the test should be considered two-dimensional rather than one-dimensional \citep[see also][for a discussion of the connection between DIF and multidimensionality]{Ack:1992,Stretal:2021:APM}.

We will now illustrate how to use the R packages \texttt{glmertree} and \texttt{psychotree} to conduct the MOB analyses. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Application Example 1: Subgroup Detection in Mixed-Effects Models}
\label{sec:TutorialMixed}


<<data_prep_mixed, echo=FALSE>>=
## Load data
data <- data.frame(haven::read_dta("data_Deming_2008_0217_MF.dta"))

## Keep only eligible children in HS or no preschool
data <- data[data$Elig2_90 == 1, ]
data <- data[data$None2_90 == 1 | data$HS2_90 == 1, ]
data <- data[data$MotherID %in% names(table(data$MotherID) >= 2), ]
data$program <- factor(ifelse(data$None2_90 == 1, "None", 
                              ifelse(data$Pre2_90 == 1, "Pre", "HS")))

## Keep only families in which children differ in HS participation
selected_family <- rowSums(table(data$MotherID, data$program) > 0) == 2
selected_family <- names(selected_family)[selected_family]
data <- data[data$MotherID %in% selected_family, ]

## Node-specific predictor
data$program <- factor(ifelse(data$None2_90 == 1, "None", 
                              ifelse(data$Pre2_90 == 1, "Pre", "HS")),
                       levels = c("None", "HS"))

## Partitioning variables

## ageAFTQ: age-adjusted AFQT
## Race_child: Hispanic=Race_Child==1; Black=Race_Child==2; White=Race_Child==3
data$Race_Child <- factor(ifelse(data$Race_Child == 1, "Hispanic", 
                                 ifelse(data$Race_Child == 2, "Black", "White")))
## family income: PermInc
## Mom's years of education: MothED
part_vars <- c("AgeAFQT", "Race_Child", "PermInc", "Height_Mom81", "MothED")
id_vars <- c("ChildID", "MotherID")

## Outcome: Peabody Picture Vocabulary Test (PPVT)
testscore_vars <- c(paste0("PPVT_Raw", 2*(43:52)), paste0("AgeTest_Yr", 2*(43:52)))

## Create long dataset
pred_vars <- "program"
HS_dat <- reshape(data[ , c(part_vars, id_vars, testscore_vars, pred_vars)], 
                  varying = list(paste0("PPVT_Raw", 2*(43:52)),
                                 paste0("AgeTest_Yr", 2*(43:52))),
                  direction = "long", 
                  v.names = c("PPVT_Raw", "AgeTest_Yr"))
HS_dat <- HS_dat[ , -which(names(HS_dat) %in% c("time", "id"))]
names(HS_dat) <- c("AFTQ", "Race", "Income", "Mom_height", "Mom_edu_yrs", 
                   "ChildID", "MotherID", "Program", "PPVT", "Age")
HS_dat <- HS_dat[complete.cases(HS_dat), ]
HS_dat$Age_orig <- HS_dat$Age
#HS_dat$Age <- sqrt(HS_dat$Age)
rownames(HS_dat) <- NULL
saveRDS(HS_dat, file = "HS_dat.Rda")
@

\subsection{Dataset}

To illustrate the use of GLMM trees, we analyze a dataset from \cite{Demi09}, who evaluated long-term benefits of participation in Head Start. Head Start is a federally funded nationwide pre-school program for children from low-income families in the United States. Participation in Head Start takes place from ages 3 through 5. \cite{Demi09} compared performance of siblings who differed in their participation in the program using data from the National Longitudinal Survey of Youth (NLSY; REF). 

The sample consists of \Sexpr{length(unique(data$MotherID))} families with at least two children, where at least one child participated in Head Start and at least one child did not participate in Head Start or any other preschool program. Data from children in those families who participated in another preschool program were excluded. As such, siblings who did not participate in Head Start serve as a natural control to assess the effects of participating in Head Start. The outcome variable comprises repeated assessments on the Peabody Picture Vocabulary Test (PPVT; REF), we will this model PPVT trajectories over time. There were an average number of \Sexpr{round(mean(table(HS_dat$ChildID)), digits = 2)} PPVT scores per child, for \Sexpr{round(100*(1-mean(table(HS_dat$ChildID) > 1)), digits = 0)}\% of the children there was only a single PPVT score available. 

%Developmental trajectories rarely if ever show a linear pattern over time; they typically show strong increases early on, which level off over time. We therefore took the square root of the child's age in years, to obtain a pattern of approximately linear increases over time. Appendix A shows PPVT scores as a function of time, both before and after the transformation. 

Our dataset contains five family characteristics: The mother's score on the Armed Forced Qualification Test (AFTQ), adjusted for age; the families' income (averaged over the years for which data was available); race (Black, Hispanic or White); mother's years of completed education; mother's height. Note that the latter variable is one that should be completely irrelevant for predicting performance on a vocabulary test; it is included here to illustrate that the GLMM tree algorithm can fruitfully distinguish signal from noise variables. The dataset comprises data from families and children for whom complete data was available. 

We load the data and inspect the first rows:

<<>>=
HS_dat <- readRDS("HS_dat.Rda")
head(HS_dat, 3)
@

We inspect the complete dataset by plotting PPVT scores against age, separated by program participation: None (black) versus Head Start (red). To show the effect of age and Head Start participation, we first fit a mixed-effects model comprising their main and interaction effects, using package \texttt{lme4}. To account for the correlation between repeated assessments on the same child, and between siblings with the same mother, we specify a random intercept for children, nested within mothers:

<<>>=
library("lme4")
lmm <- lmer(PPVT ~ Program*Age + (1|MotherID/ChildID), data = HS_dat)
@

The results are presented in Figure~\ref{fig:global_lmm}, which shows that children participating in Head Start show slightly higher performance than their non-participating siblings and that this difference is slightly diminished but persistent over time. This result agrees with the findings of \cite{Demi09}. The code for the figure is omitted here for space considerations, because we want to focus on fitting and interpreting GLMM trees. Code for exact replication of the results presented here is provided in the Supplementary Materials. 

\begin{figure}%
\caption{}
\begin{subfigure}{.7\textwidth}
<<echo=FALSE, fig=TRUE>>=
beta <- fixef(lmm)
plot(jitter(HS_dat$Age), HS_dat$PPVT, col = HS_dat$Program,
     cex = .5, #cex.lab = .7, cex.axis=.7, cex.main = .7,
     xlab = "Age (years)", ylab = "PPVT score", xaxt = "n",
     main = paste0("Global Model (", length(unique(HS_dat$ChildID)), " children)"))
abline(a = beta["(Intercept)"], b = beta["Age"], col = "black")
abline(a = beta["(Intercept)"]+beta["ProgramHS"], 
       b = beta["Age"]+beta["ProgramHS:Age"], col = "red")
axis(1, #at=sqrt(c(3, 5, 8, 12, 17)), labels=c(3, 5, 8, 12, 17), 
     cex.axis = .8)
@
\end{subfigure}
\label{fig:global_lmm}
\end{figure}%

\FloatBarrier
\subsection{Linear mixed effects model tree}

Next, we test whether the intercepts and slopes of the two regression lines differ as a function of the partitioning variables, using function \verb|lmertree| from R package \textbf{glmertree}:

<<>>=
library("glmertree")
HS_tree <- lmertree(PPVT ~ Program*Age | (1|MotherID/ChildID) | AFTQ + Race + 
                      Income + Mom_edu_yrs + Mom_height, 
                    data = HS_dat, cluster = MotherID, minsize = 250)
@

With the first argument, we specified the model \verb|formula|, which has three parts separated by vertical bars: The left part (\verb|PPVT ~ Program*Age|) specifies the response variable, followed by a tilde (\verb|~|) and the fixed-effects predictors of relevance. The middle part (\texttt{1|MotherID/ChildID}) specified the random effect: Repeated PPVT assessments are nested within children, which are nested within mothers. The right part (\verb|AFTQ + Race + Income + Mom_edu_yrs + Mom_height|) specified the partitioning variables: covariates that may possibly affect the values of the fixed-effects parameters. 

With the second argument, we specified the dataset which contain the variables. With the \texttt{cluster} argument, we specified that the partitioning variables are measured at the level of the mothers. As a result, parameter stability tests will be performed on the appropriate level \citep{FokkyZeil23}. Not specifying the \texttt{cluster} argument would result in use of the default observation-level parameter stability tests, which would yield inflated type-I error rates. That is, it could result in detection of spurious subgroups. Finally, because we want to retain large enough subgroups, we specified that the minimum number of observations in a terminal node should be 250.

Next, we plot the tree. With multiple fixed-effects predictors of interest, the default plots may become too crowded or difficult to interpret. We therefore specify \verb|type = "simple"| to facilitate interpretation, and using the \verb|nodesize_level| argument, we specified that the sample size printed above every terminal node should count the number of children, not the number of individual observations:

<<eval=FALSE>>=
plot(HS_tree, type = "simple", nodesize_level = 2)
@

\begin{figure}%
\caption{}
<<fig=TRUE, echo=FALSE, fig.width=9, fig.height=4>>=
plot(HS_tree, type = "simple", which = "tree", gp = gpar(cex = .5), nodesize_level = 2)
@

\vspace*{-3cm}

<<fig=TRUE, echo=FALSE, width=7.6, height=2.5>>=
nodes <- predict(HS_tree, type = "node")
beta <- coef(HS_tree)
par(mfrow = c(1, 4))
for (node in sort(unique(nodes))) {
  plot(HS_dat$Age[nodes == node], HS_dat$PPVT[nodes == node],
       col = HS_dat$Program[nodes == node],
       cex = .5, cex.lab = .7, cex.axis=.7, cex.main = .7,
       xlab = "Age", ylab = "PPVT", #xlim = c(sqrt(3), sqrt(20)), 
       ylim = c(0, 150))
  abline(a = beta[as.character(node), "(Intercept)"], b = beta[as.character(node), "Age"], col = "black")
  abline(a = beta[as.character(node), "(Intercept)"] + beta[as.character(node), "ProgramHS"], 
         b = beta[as.character(node), "Age"] + beta[as.character(node), "ProgramHS:Age"], col = "red")
}
@
\label{fig:lmm_tree}
\end{figure}%

The resulting tree is presented in Figure~\ref{fig:lmm_tree}. Below each terminal node, we plotted the observations and the two regression curves given by the coefficients in that node. The first split was made based on the AFTQ variable, which represents the mother's score on the Armed Forced Qualification Test, adjusted for the age at which they completed the test. The group with higher mother's AFTQ scores is further split based on race. The Black and Hispanic group is further split based on income. 

<<echo=FALSE, results=tex>>=
coefs <- round(coef(HS_tree), digits = 2)
design_df <- data.frame(.tree = rep(sort(unique(HS_tree$data$.tree)), each = 2),
                        Program = factor(rep(c("None", "HS"))))
design_df$`PPVT at age 6` <- predict(HS_tree$lmer, newdata = cbind(design_df, Age = sqrt(6)), 
                                     re.form = ~0)
design_df$`PPVT at age 18` <- predict(HS_tree$lmer, newdata = cbind(design_df, Age = sqrt(18)), 
                                      re.form = ~0)
names(design_df)[1] <- "Node"
knitr::kable(design_df, format = "latex", digits = 2, 
             caption = "Predicted PPVT scores at different ages.", 
             label = "predictions", centering = FALSE,
             linesep = "", # linesep command suppressess addlinesep every 5 rows
             row.names = FALSE, escape=TRUE, align = c("cccc"), booktabs = TRUE)
@

To aid interpretation of the coefficients in Figure~\ref{fig:lmm_tree}, Table~\ref{tab:predictions} provides predicted PPVT scores for each of the groups and programs at ages 6 and 18. All nodes show a modest benefit of Head Start participation at age 6, about 3-4 points on the PPVT. This benefit remains the same over time for the group with lower mother's AFTQ scores. The benefit increases over time for White children with higher mother's AFTQ scores. Strikingly,\todo{possibly less extreme word like: Interestingly,} the benefit decreases over time for Black and Hispanic children with higher mother's AFTQ scores. These results correspond to the conclusions of \cite{Demi09}. 


\todo[inline]{With different specification of the GLMM tree (no square root, parameter stability tests at mom level, random intercepts of children nested within mothers) the results have changed, still need to adjust description}.

\todo[inline]{Marjolein add here comment that typically affect of age is typically strongest; take out next part, but comment on it in discussion: CV would be a technique to see how stable results are, cite something about CV in general}

%To evaluate whether the detected subgroups indeed contribute to better predictions, we used cross validation. We \edc{randomly} separated the \Sexpr{length(unique(HS_dat$MotherID))} families in the dataset into ten equally-sized folds. We took nine of the ten folds as a training dataset on which we fitted two models: an LMM tree \edc{like the one shown above} (i.e., an LMM with subgroups \todo{necessary? confused me}) and an LMM without subgroups (comprising main and interaction effects of age and Head Start participation, and a random intercept with respect to child). We evaluated performance on the remaining folds by computing and evaluating accuracy of the predictions. \todo{explain MSE and $R^2$} We repeated this procedure ten times, so that all folds were used as a test set once. The results are presented in Table~\ref{tab:performance}, which shows that LMM trees generalize well: They provide better predictive accuracy compared to LMMs, while implementing only few splits. 





<<cv_lmm_tree, echo=FALSE, results=tex>>=
results <- data.frame(MSE_tree = rep(NA, times = 10), 
                      MSE_lmm = rep(NA, times = 10),
                      tree_size = rep(NA, times = 10))

set.seed(42)
folds <- sample(1:10, replace = TRUE, size = length(unique(HS_dat$MotherID)))
HS_dat$fold <- NA
for (mom in unique(HS_dat$MotherID)) {
  HS_dat$fold[HS_dat$MotherID == mom] <- folds[unique(HS_dat$MotherID) == mom]
}
for (i in 1:10) {
  ## Fit models on training data
  lmmt <- lmertree(PPVT ~ Age*Program | ChildID | AFTQ + Race + Income + 
                     Mom_height + Mom_edu_yrs, 
                   cluster = ChildID, data = HS_dat[HS_dat$fold != i, ], 
                   minsize = 225)
  lmm <- lmer(PPVT ~ Age*Program + (1|ChildID), 
              data = HS_dat[HS_dat$fold != i, ])
  
  ## Evaluate performance on test data
  results$tree_size[i] <- (length(lmmt$tree)-1)/2
  preds <- predict(lmmt, newdata = HS_dat[HS_dat$fold == i, ], re.form = ~0)
  results$MSE_tree[i] <- mean((preds - HS_dat[HS_dat$fold == i, "PPVT"])^2)
  preds <- predict(lmm, newdata = HS_dat[HS_dat$fold == i, ], re.form = ~0)
  results$MSE_lmm[i] <- mean((preds - HS_dat[HS_dat$fold == i, "PPVT"])^2)
}
res_df <- data.frame(method = c("LMM tree", "LMM"),
                     MSE = colMeans(results[1:2]),
                     SD = sapply(results, sd)[1:2],
                     `number of splits` = c(colMeans(results)[3], NA),
                     R2 = colMeans(1 - results[ , 1:2] / var(HS_dat$PPVT)))
knitr::kable(res_df, format = "latex", digits = 3, 
             caption = "Cross-validated performance of LMMs and LMM trees.", 
             label = "performance", centering = FALSE,
             linesep = "", # linesep command suppressess addlinesep every 5 rows
             row.names = FALSE, escape=TRUE, align = c("ccccc"), booktabs = TRUE)
@




\newpage
\FloatBarrier
\section{\edc{Application Example 2}: Subgroup Detection in Rasch Models}
\label{sec:TutorialRasch}

\SweaveInput{Raschtree_MH.Rnw}

\FloatBarrier
\section{Discussion}

\todo{next paragraph moved to here, where exactly?}
An alternative framework for detecting groups of persons with different model parameters is mixture modelling \citep[see, e.g.,][in the context of Rasch modelling]{AyalySant17,FriStrZei:2015:EaPM}. Mixture modelling aims at identifying latent classes of persons with different properties. It can also be combined with covariates. A comparison of both approaches is given by \citet{FriStrZei:2014}.

In this tutorial paper we have outlined the rationale of the MOB algorithm and how it can be used for detecting parameter differences in mixed-effects and Rasch models. The main advantage of MOB is that it can detect groups of persons with different model parameters in a data-driven manner. This makes it more flexible for detecting differences that were not hypothesized by the researcher. For example in DIF analysis, it is often the case that obvious sources of DIF have already been avoided by the content experts during item creation. Any remaining DIF is unexpected. Therefore, any DIF detection approach that relies on the researchers correctly specifying the exact groups of persons that exhibit DIF can miss DIF if it is associated with other (combinations of) covariates or other cutpoints than the ones investigated. In this sense, the more exploratory approach of Rasch trees has higher statistical power to detect DIF in previously unknown groups \citep{StrKopZei:2015:P}. 

The same holds for parameter differences in mixed-effects models, where the substantial hypotheses are typically about the fixed effects of the interventions, not necessarily about all possible subgroup-specific .... \todo{Marjolein add if this makes any sense here}


For our two application examples, we could show that MOB can detect ..., performs automated variable selection ...



\subsection{Limitations}

Due to the exploratory nature of MOB, it does not allow for hypothesis tests, as there is no valid way to account for the exploratory searching of the subgroups. Thus, even if detected subgroups are substantively meaningful or differences are large, if researchers want to ascertain statistical significance of the subgroup differences, this should be done on new data using confirmatory techniques.

\todo[inline]{Mention shortcomings.}


\todo[inline]{Write about future work.}

sample size dependence, similar ideas like MH trees also possible for other MOB? \todo[inline]{possibly Mirka add something about visual interpretation? because effect sizes in mixed effects models are not very clear}

The Rasch tree method has been extended to include an effect size measure that can stop the tree from growing if the effect size is non-substantial, but also supports researchers in interpreting the tree's results with respect to whether DIF effects are negligible, medium, or large. At the same time, effect sizes are less straightforward to calculate and interpret in linear mixed models, and therewith in the linear mixed effects model tree. A remedy to this issue can be interpretation techniques, such as partial dependence plots or individual conditional expectations plots. These interpretation techniques support researchers in gauging the size of the effect of predictor variables visually by depicting the predicted value of the machine learning method as a function of the value of the predictor variable(s). A comprehensive introduction, tutorial, and discussion into interpretation techniques for machine learning methods is given by \cite{Molnar2019} and \cite{Henninger2022c}. 
\todo[inline]{This is my (Mirka) suggestion for the interpretation techniques. Please feel free to add, edit, delete, however you prefer!}

The Mantel-Haenszel effect size measure for DIF will be integrated in the \texttt{psychotools} package. 

\todo{Mention that the exclusion of random effects from partitioning can both be advantage and disadvantage. In future work, use of parameter stability tests for random-effects parameters of Ting and Ed will be explored.}

\todo{Mention that transformations chosen in growth curve modeling may yield different effects. In future work, recursive partitioning of GAMs will be explored to obviate the need for manual transformation of the predictors.}

\bibliography{_MOB_paper}

\todo{Add DOIs.}

\newpage
\appendix



\section{Appendix A: Association between Age and PPVT scores before and after transformation}
\label{sec:AppendixA}


\begin{figure}
\caption{Association between Age and PPVT scores before (left) and after (right) transformation.}
<<echo=FALSE, fig=TRUE, width=6, height=4>>=
par(mfrow = c(1, 2))
plot(HS_dat$Age, HS_dat$PPVT, cex = .7, xlab = "Age", ylab = "PPVT",
     cex.lab = .7, cex.axis = .7)
plot(sqrt(HS_dat$Age), HS_dat$PPVT, ylab = "PPVT", xlab = "Age (square root)", 
     cex = .7, cex.lab = .7, cex.axis = .7)
@
\end{figure}


\end{document}