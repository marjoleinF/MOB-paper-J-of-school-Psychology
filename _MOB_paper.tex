\documentclass[doc,floatsintext,natbib]{apa7}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{doi}
\usepackage{booktabs}
%\usepackage[dvipsnames]{xcolor}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{setspace} % for adjusting line spread
\usepackage{hyperref} 


\renewcommand{\doitext}{} % prevents superfluous addition of "doi:" in references
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % uses symbols instead of numbers for footnotes
\usepackage[colorinlistoftodos, textsize=footnotesize]{todonotes}
%%\todo[inline, color=green!40]{This is a green inline comment.}

%text from Marjolein edited by Caro
\newcommand{\edc}[1]{\textcolor{blue}{#1}}

\linespread{1.5}


\title{One Model May Not Fit All: Subgroup Detection Using Model-Based Recursive Partitioning}
\shorttitle{Subgroup Detection Using Model-Based Recursive Partitioning}

\authorsnames{ }
\authorsaffiliations{ }

\abstract{Model-based recursive partitioning \citep[MOB,][]{ZeilyHoth08} is a flexible framework for detecting subgroups of persons showing different effects in a wide range of parametric models. It provides a versatile tool for detecting and explaining heterogeneity in, for example, intervention studies. In this tutorial paper, we provide an introduction to the general MOB framework. In two specific case studies, we show how MOB-based methods can be used to detect and explain heterogeneity in two widely-used frameworks in educational studies: the generalized linear mixed model (GLMM) and item response theory (IRT). In the first case study, we show how GLMM trees \citep{FokkySmit18} can be used to detect subgroups with different parameters in mixed-effects models. We apply GLMM trees to longitudinal data from a study on the effects of the Head Start pre-school program, to identify subgroups of families where children show comparatively larger or smaller gains in performance. In a second case study, we show how Rasch trees \citep{StroyKopf15} can be used to detect subgroups with different item parameters in IRT models, i.e.~differential item functioning (DIF). DIF should be investigated before using test results for group comparisons. We show how a recently developed stopping criterion \citep{HennyDeba23} can be used to guide subgroup detection based on DIF effect sizes.\\}



\usepackage{Sweave}
\begin{document}
\input{_MOB_paper-concordance}
\renewenvironment{Schunk}{\small}{}



\begin{titlepage}
   \begin{center}
       \vspace*{1cm}

       \textbf{\large One Model May Not Fit All}\\

       \vspace{0.5cm}
        \large Subgroup Detection Using\\Model-Based Recursive Partitioning\\
            
       \vspace{1.5cm}

       \textbf{Marjolein Fokkema$^1$, Mirka Henninger$^{2,3}$ and Carolin Strobl$^2$}\\
       $^1$Leiden University, $^2$University of Zurich, $^3$ University of Basel

       \vfill

            
       \vspace{0.8cm}
     

   \end{center}
\end{titlepage}


\maketitle



\newpage
\section{Introduction}
\label{sec:Introduction}

Model-based recursive partitioning \citep[MOB;][]{ZeilyHoth08} % simplified wording
allows for detecting subgroups of persons that differ in the parameters of a statistical model. To find the subgroups, MOB uses \textit{recursive partitioning}, a technique that was first popularized by the classification and regression trees \citep[CART; ][]{Breetal:1984} algorithm. In CART, the aim is to detect groups of persons, defined by specific (combinations of) covariate values, that differ in their mean on a response variable. Detecting such subgroups may be of interest in many studies in school and developmental psychology. For example, recursive partitioning methods have been used to identify adolescents at risk of committing crime \citep{FrityHaup19} and experiencing corporal punishment \citep{StemyHein19}, and to identify predictors of math ability \citep{DingyZhao19}.

Like CART, MOB also identifies groups of persons defined by (combinations of) covariate values, but the groups can differ in a wider range of parameters, instead of only the mean. For example, MOB can be used to identify children that show stronger or weaker effects of an intervention \citep{ChunyAnso21}, or stronger or weaker gains in academic skills over time \citep{FokkyZeil24}. The framework of MOB is very flexible and can be applied to a wide range of statistical models, such as %simplified wording 
linear and logistic regression \citep{KopAugStr:2013,ZeilyHoth08} or models for paired-comparison data \citep{StrWicZei:2011:JoEaBS,WiedyFrick21}. In this article, we will highlight two specific MOB methods that may be particularly relevant for research in school psychology: MOB for mixed-effects models \citep{FokkySmit18} and MOB for Rasch measurement and Item Response Theory models \citep[IRT,][]{StroyKopf15, KomStrZei:2017:EaPM, HennyDeba23}. 

Mixed-effects models become relevant whenever data are collected in longitudinal or nested data structures. For example, when children are tested at several time points, observations at each timepoint are nested in children; when children from different classes from different schools participate in a study, children are nested in classes, which are in turn nested in schools. In the section \hyperref[sec:glmmtree]{MOB for Subgroup Detection in Mixed-Effects Models}, we present an \hyperref[sec:TutorialMixed]{example application} to illustrate how MOB can be used to detect subgroup-specific intervention effects, while taking into account the nested data structure. In this example, the focus is on detecting subgroups with different parameters of a \textit{regression} model. We thus assume that the psychological test score(s) of interest have already been validated. 

In the section~\hyperref[sec:raschtree]{MOB for Detecting DIF in Measurement Models}, we go back in the research process to the point where a new psychological test has been administered to a validation sample. Here, we focus on detecting subgroups with different parameters of a \textit{measurement} model. Validity assessment requires that we make sure that test results are comparable, for example between children of different genders. If certain items show different measurement parameters between groups, this may put certain groups at a relative disadvantage. These items are said to violate measurement invariance or exhibit differential item functioning (DIF). In order to test for DIF in the framework of Item Response Theory or Rasch modelling \citep{AnthyDiPe16,DebStrZei:2022:CRC,Mall97}, the item parameters are compared between groups of persons. This can be done in a way that allows to detect DIF, while accounting for possible true group differences in ability. Traditional approaches for testing DIF require the groups to be pre-specified in order to test for DIF. In an \hyperref[sec:TutorialRasch]{application example} we show how MOB flexibly allows to detect groups with different item parameters in a data-driven way. 

In the following section, we first give a short introduction into the algorithm and statistical concepts behind MOB. Readers interested in learning more about its predecessor method, classification and regression trees, are referred to the introduction by \citet{StrMalTut:2009:PM}. Our aim here is to provide an informal introduction and to illustrate the relevance of these methods for studies in school psychology. For readers interested in the more formal details of MOB and the extensions presented here, relevant references are provided throughout the manuscript. 

\section{The MOB Algorithm}

The main rationale of MOB is that one global model may not fit all observations in a dataset equally well. In many studies, additional covariates may be available. It may then be possible to uncover subgroups defined by these covariates, and obtain better-fitting models in each of those subgroups \citep{ZeilyHoth08}. 



We illustrate this idea using a simple, simulated toy dataset, comprising 250 observations and four variables: A continuous response variable $y$ (e.g., a total score for behavioral difficulties), and three covariates, $x_1$ (e.g., age), $x_2$ (e.g., a reading comprehension score) and $x_3$ (e.g., gender, with levels male, female and non-binary), as possible partitioning variables. To keep the example simple, we apply MOB to a very basic global model, which comprises % simplified wording
only an intercept (or mean). It would also be possible to use, for example, a linear or logistic regression model as the global model. Figure~\ref{fig:toy}, left, shows the distribution of $y$ in a boxplot. Obviously, a global intercept or mean would not describe all observations equally well, because there is quite some unexplained variation around the sample mean of 5.91.


\begin{figure}[h]
\caption{Left: Univariate distribution of the response variable. Right: Tree with group-specific distributions of the response variable in the terminal nodes.}
\begin{subfigure}[][][t]{.4\textwidth}
\includegraphics{_MOB_paper-003}
\end{subfigure}
\begin{subfigure}[][][b]{.7\textwidth}
\includegraphics{_MOB_paper-004}
\end{subfigure}
\label{fig:toy}
\end{figure}

\begin{figure}[h]
\caption{Top row: Residuals ordered by the values of the partitioning variables. Bottom row: Summed residuals (cumulative sums for continuous predictors, sums by level for categorical predictors).}
\begin{subfigure}[][][t]{1.2\textwidth}
\includegraphics{_MOB_paper-006}

\includegraphics{_MOB_paper-007}
\end{subfigure}
\label{fig:efp}
\end{figure}


To detect possible subgroups with different values for the parameters, MOB cycles iteratively through the following steps:

\begin{enumerate}
\setlength\itemsep{0.25em}
\item The model parameters are first estimated jointly for all persons in the current node, starting with the root node containing the full sample.
\item Structural change in the model parameters is assessed with respect to each available covariate.
\item If there is significant structural change, the observations in the current node are split using the covariate associated with the strongest change.
\item Steps 1--3 are repeated recursively in each resulting node until there is no more significant structural change (or the groups become too small).
\end{enumerate}

We applied MOB to the observations in the left panel of Figure~\ref{fig:toy}, specifying $x_1$, $x_2$ and $x_3$ as potential partitioning variables. In Step 1, the model parameters (here: the intercept) are estimated in the root node, which contains all observations. %changed
In Step 2, the model's \emph{scores} are computed. In case of an intercept-only model, these are simply the residuals. The scores are ordered with respect to the possible partitioning variables, as shown in the top row of Figure~\ref{fig:efp}. The scores should randomly fluctuate around their mean of zero, but the top row of Figure~\ref{fig:efp} suggests systematic deviations: there seems to be an association with $x_1$ and $x_2$, and little or no association with $x_3$. 

MOB captures systematic deviations in the scores (i.e., structural change) by computing cumulative sums. The cumulative sums are depicted in the bottom row of Figure~\ref{fig:efp}. To decide in Step 3 whether to split the node, and if so, which variable to split on, a test statistic is computed based on the cumulative sums of each partitioning variable; full details on the computation are provided in \citep{ZeilyHoth08}. In the current example, the structural change tests revealed that $x_1$ was most strongly associated with instabilities in the intercept and it was thus selected for splitting in Step 3. 

In Step 4, the procedure is repeated in each of the resulting subgroups. The resulting tree is shown in the right panel of Figure~\ref{fig:toy}. In the right subgroup (node 3), additional significant instability was detected with respect to $x_2$, a categorical covariate. Again, the cutpoint for $x_2$ was selected so that the two resulting subgroups exhibit an as large as possible parameter difference. In the left subgroup (node 2), no further splits were created, because none of the three covariates were significantly associated with any further instability in this subgroup. The same held for nodes 4 and 5, so that the third covariate $x_3$ was never selected for splitting. 

The $p$ values resulting from the structural change tests of each split are depicted in the corresponding nodes. By default, the $p$ values are Bonferroni corrected, to account for the fact that the tests are performed for all potential partitioning variables. After selecting $x_1$ for splitting, the cutpoint is selected so that the two resulting subgroups exhibit the strongest parameter differences, while the observations within the subgroups are as similar as possible.

The subgroup-specific distributions of the response variable are depicted in the terminal nodes at the bottom of the tree. The fact that the tree in Figure~\ref{fig:toy} displays more than one terminal node confirms that one global model for all observations does not appropriately capture the patterns in the data.  The tree also shows shows that, out of the three covariates that were presented to the algorithm, only two were selected for splitting. This automatic variable selection is an important characteristic of the CART and MOB algorithms. 

The means (intercepts) in the terminal nodes are 4.42, 4.78 and 9.17, respectively. While the means and distributions in nodes 2 and 4 appear very similar, the mean in node 5 is obviously higher. Substantively, this tree suggests an interaction effect: The effect of $x_2$ depends on the level of $x_1$ (vice versa). If $x_1$ were age and $x_2$ gender, this would indicate that gender differences only start to occur at a later age.   


%The subgroups are defined by the covariates that are used for splitting and together with cutpoints in those covariates. For example, in Figure~\ref{fig:lmm_tree} for the metric covariates \texttt{AFTQ} and \texttt{Income} a numeric cutpoint has been selected by the MOB algorithm to separate the groups. The algorithm has identified this cutpoint as being the location of the strongest parameter difference between the two resulting groups. The categorical covariate \texttt{Race}, which was coded in three categories in this example, has been divided into two groups of categories: Black and Hispanic vs.~White. For a binary covariate, \todo{like ... -- add name if is there one in the other example?} 
%on the other hand, there is only one possible cutpoint, namely between the two categories. 

%While in Figure~\ref{fig:lmm_tree} each terminal node contains a linear mixed-effecs model with a group-specific effect for the Head Start intervention, in Figures~\ref{fig:math_tree} and \ref{fig:read_tree} each terminal node contains the group-specific item parameters of a Rasch model for the respective test items. 
%The fact that the Figures display more than one terminal node already means that one joint mixed-effects or Rasch model was not appropriate to describe the pattern in the data. 

%Figure~\ref{fig:lmm_tree} also shows that, out of the  covariates that were presented to the algorithm (\texttt{AFTQ}, \texttt{Race}, \texttt{Income},\texttt{Mom\_edu\_yrs}, and \texttt{Mom\_height}), only \texttt{AFTQ}, \texttt{Race}, and \texttt{Income} were actually selected for splitting. Variables are selected based on a statistical test for parameter differences, also termed a test for structural change. Structural change is present if, for example, the model parameters systematically differ for children from lower vs.~higher income families. At each node, the algorithm will select the covariate showing the strongest structural change as the next splitting variable. Within this variables, the optimal cutpoint is chosen in a separate step.

%While it is also possible to create more than two groups in each split \citep{KimLoh:2001,Qui:1993}, binary splitting algorithms are typically preferred because they lead to more concise trees. % Think we can leave this sentence out, I'm not convinced readers will consider that other than binary splits may be implemented.


There are three further important characteristics of tree and MOB algorithms that we would like to mention here: The first characteristic is the type of variable and cutpoint selection an algorithm employs. Traditional classification and regression tree algorithms, like those of \citet{Breetal:1984} and \citet{Qui:1993}, performed variable and cutpoint selection in one step, which leads to an undesirable behavior called variable selection bias. That is, the traditional algorithms prefer variables offering more possible cutpoints in the selection process -- regardless of their true information content. An algorithm that still has this problem is, for example, the \texttt{rpart} algorithm in R, based on the original CART algorithm by \citet{Breetal:1984}. More modern algorithms for classification and regression trees have solved this problem and offer unbiased variable selection, such as \texttt{QUEST} \citep{LohShi:1997}, which is available in SPSS, and \texttt{ctree} \citep{Hotetal:2006}, which is available in \texttt{R} in packages \texttt{party} and \texttt{partykit} \citep{partykit:pkg}. The latter forms the basis for all MOB approaches presented in this paper. For more details on the statistical theory behind unbiased classification and regression trees and MOB, see \citet{Hotetal:2006}, \citet{StrMalTut:2009:PM} and /citet{StroyKopf15}.  

The second characteristic relates to the way a tree or MOB algorithm stops splitting: Modern algorithms for classification and regression trees and MOB use a criterion of statistical significance to stop splitting. Once there are no more covariates that show a significant structural change in any node, splitting is halted. In this way, the  algorithm selects only those partitioning variables that are relevant for distinguishing the groups (i.e., it performs automatic variable selection, as illustrated in the right panel of Figure~\ref{fig:toy}). Moreover, the trees will not grow as large as possible, but splitting is stopped when no more significant structural change is detected. While traditional tree algorithms like those of \citet{Breetal:1984} and \citet{Qui:1993} grew very large trees and then cut them back (so called \textit{pruning}), modern tree and MOB algorithms employ significance tests as stopping criteria (and some can also use effect size measures, as we will see in \hyperref[sec:TutorialRasch]{Application Example 2}). This allows for stopping tree growing as soon as no significant differences can be detected anymore. Other stopping criteria are based on the number of persons in the terminal nodes. These criteria ensure that the sample sizes in the terminal nodes are large enough to estimate the statistical model in each terminal node. For reasons of interpretability, users may specify even higher requirements for the sample sizes of terminal nodes.

The third characteristic of classification and regression trees as well as MOB is that the entire structure identified by the trees does not have to be pre-specified by the researcher in a confirmatory manner, but is learned from the data in an exploratory manner. This is a key feature of the MOB approach that makes it very flexible and sets it apart from purely parametric approaches, where only those main effects and interactions that are explicitly included in the specification of the model are considered. While there are phases in psychological and educational research where it is very important to specify hypotheses a-priori and test them in a confirmatory manner, in early stages of research exploratory methods are an important addition to the statistical toolbox for researchers. Still, an important challenge for the researcher remains: To specify the parametric model of interest, to specify the set of possible partitioning variables and to choose the settings of the MOB algorithm. The current paper aims to provide guidance. 

Next, we discuss two specific methods that make use of the general MOB framework introduced above. The methods allow for partitioning two types of statistical models that are particularly relevant for school psychology research: Mixed-effects models for repeated measures or nested data structures and measurement models for validating psychological and educational tests. 


\section{MOB for Subgroup Detection in Mixed-Effects Models}
\label{sec:glmmtree}

Mixed-effects models contain two types of effects: Fixed and random effects. Fixed effects are typically used to capture population-averaged effects, while random effects are used to capture inter-individual variation around these fixed effects. In many studies, researchers are specifically interested in testing hypotheses relating to the population-averaged effects, while the random effects are included in the model to properly account for inter-individual variation, and correlations between observations within the same unit \citep{RaudyBush02}.

GLMM trees combine MOB and generalized linear mixed-effects models (GLMMs) and were introduced by \cite{FokkySmit18}. While the 'standard' MOB trees \citep{ZeilyHoth08} allow for subgroup detection in fixed-effects GLMs, GLMM trees additionally estimate and account for random effects and can thus be used for subgroup detection in mixed-effects regression models. The GLMM tree algorithm only targets structural change in the fixed-effects parameters. The random-effects parameters are specified as usual and assumed constant; that is, they are estimated using all observations in the dataset. The resulting subgroups will differ in their estimates for the fixed-effects coefficients only. As such, GLMM trees allow for detecting subgroups in multilevel models that differ with respect to any (set of) fixed-effects parameters of interest. For example, users may be interested in detecting subgroups with different means \citep{FokkyEdbr21}, but also differential effects of treatment \citep{FokkySmit18} or differential growth over time \citep{FokkyZeil24}, to name but a few examples. 

The \texttt{R} functions for fitting GLMM trees are \texttt{lmertree} and \texttt{glmertree}. The former assumes a normally distributed response, while the latter supports, for example, binomial or count responses. The following section will illustrate the usage of \texttt{lmertree}. Mathematical and computational details of the GLMM tree algorithm are described in \cite{FokkySmit18} and \cite{FokkyZeil24}. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Application Example 1: Detecting Differential Effects of Head Start}
\label{sec:TutorialMixed}



\subsubsection{Dataset}

To illustrate the use of GLMM trees, we analyze a dataset from \cite{Demi09}, who evaluated long-term benefits of participation in the Head Start program. Head Start is a federally funded nationwide pre-school program for children from low-income families in the United States. Participation in Head Start takes place from ages 3 through 5. \cite{Demi09} compared performance of siblings who differed in their participation in the program using data from the 1979 cohort of the National Longitudinal Survey of Youth \citep{NLSY}. 

The sample consists of 273 families with at least two siblings, where at least one sibling participated in Head Start and at least one sibling did not participate in Head Start or any other pre-school program. Data from children who participated in another pre-school program were excluded. The family structure allows siblings who did not participate in Head Start to serve as a natural control to assess the effects of participating in Head Start. The outcome variable comprises repeated assessments on the Peabody Picture Vocabulary Test \citep[PPVT; ][]{DunnyDunn81}, for which we model trajectories over time. On average, there were 2.14 PPVT scores per child, for 71\% of the children there were $>1$ PPVT scores available. 

%Developmental trajectories rarely if ever show a linear pattern over time; they typically show strong increases early on, which level off over time. We therefore took the square root of the child's age in years, to obtain a pattern of approximately linear increases over time. Appendix A shows PPVT scores as a function of time, both before and after the transformation. 

Our dataset contains five family characteristics: Mother's score on the Armed Forced Qualification Test (AFTQ), adjusted for age; family income (averaged over the years for which data was available); race (Black, Hispanic or White); mother's years of completed education; mother's height (computed as feet x 100 + inches). Note that the latter variable is one that should be completely irrelevant for predicting performance on a vocabulary test; it is included here to illustrate that the GLMM tree algorithm can fruitfully distinguish signal from noise variables. The dataset analyzed here only comprises data from families and children for whom complete data was available. 

We load the data and inspect the first rows:

\begin{Schunk}
\begin{Sinput}
 load("HS_dat.Rda")
 head(HS_dat, 3)
\end{Sinput}
\begin{Soutput}
       AFTQ     Race   Income Mom_height Mom_edu_yrs ChildID MotherID Program
1  3.478122 Hispanic 37731.07        502          12   20502      205      HS
2  3.478122 Hispanic 37731.07        502          12   20501      205    None
3 15.964368    Black 16119.13        504          10   22403      224    None
  PPVT Age Age_orig
1   18   4        4
2   48   7        7
3   69   7        7
\end{Soutput}
\begin{Sinput}
 hist(tapply(HS_dat$Mom_height, HS_dat$MotherID, mean))
\end{Sinput}
\end{Schunk}

We first inspect the complete dataset by plotting PPVT scores against age, separated by program participation: None versus Head Start. To show the global effect of age and Head Start participation, we first fit a mixed-effects model comprising their main and interaction effects, using package \texttt{lme4}. This would also be the model fitted in the root node of the GLMM tree:

\begin{Schunk}
\begin{Sinput}
 library("lme4")
 lmm <- lmer(PPVT ~ Program*Age + (1|MotherID/ChildID), data = HS_dat)
\end{Sinput}
\end{Schunk}

We specified the fixed main and interaction effects of `Program` and `Age` as `Program*Age`. The random effects were specified as `(1|MotherID/ChildID)`. The \texttt{1} specifies that a random intercept should be estimated and \texttt{MotherID/ChildID} specifies that every child and every mother should obtain a random intercept, so as to account for correlations between repeated assessments of the same child, and correlations between children of the same mother. The forward slash \texttt{/} indicates that the children are 'nested' within mothers; that is, that there are multiple children per mother, but only one mother per child.

The results are presented in Figure~\ref{fig:global_lmm}, which shows that children participating in Head Start show slightly higher performance than their non-participating siblings and that this difference persists but slightly diminishes over time. This result agrees with the findings of \cite{Demi09}. Figure~\ref{fig:global_lmm} also shows that the effect of age on performance is very strong, which is typical for young children growing up to adults, while the effect of intervention (Head Start) is small in comparison. Note that the \texttt{R} code for creating the figure is omitted here, because we want to focus on fitting and interpreting GLMM trees. Code for exact replication of the results presented here is provided in the Supplementary Materials. 



\begin{figure}%
\caption{}
\begin{subfigure}{.7\textwidth}
\includegraphics{_MOB_paper-011}
\end{subfigure}
\label{fig:global_lmm}
\end{figure}%

\FloatBarrier
\subsubsection{Fitting a GLMM tree}

We now test whether the intercepts and slopes of the two regression lines differ as a function of the partitioning variables, using function \verb|lmertree| from R package \textbf{glmertree}:

\begin{Schunk}
\begin{Sinput}
 library("glmertree")
 HS_tree <- lmertree(PPVT ~ Program*Age | (1|MotherID/ChildID) | AFTQ + Race + 
                       Income + Mom_edu_yrs + Mom_height, 
                     data = HS_dat, cluster = MotherID, minsize = 250)
\end{Sinput}
\end{Schunk}

With the first argument, we specified the model \verb|formula|, which has three parts separated by vertical bars: The left part (\verb|PPVT ~ Program*Age|) specifies the response variable, followed by a tilde (\verb|~|) and the fixed-effects predictors of relevance. The middle part (\texttt{1|MotherID/ChildID}) specified the random effects, which are identical to the random effect in the earlier mixed-effects model. The right part (\verb|AFTQ + Race + Income + Mom_edu_yrs + Mom_height|) specified the partitioning variables: covariates that may possibly affect the values of the fixed-effects parameters. 

With the second argument, we specified the dataset which contain the variables. With the \texttt{cluster} argument, we account for the level at which the partitioning variables ae measured. In this example, the partitioning variables are measured at the mother (or family) level, which we indicated by specifying \texttt{cluster = MotherID}. As a result, the parameter stability tests will employ so-called clustered covariances \citep{ZeilyKoll20}. Not specifying the \texttt{cluster} argument would result in use of the default observation-level covariances in the parameter stability tests, inflating type-I error rates when partitioning variables are measured on a higher level \citep{FokkyZeil24}. 
 
% Mirka: And if I remember correctly, one critical point was also that in longitudinal analysis, we want to avoid that a person is ``split up'' in several terminal nodes, and that the cluster argument also helps for that? If that's correct, this could also be a good place to add a sentence about this advantage? Marjolein: The splitting up is avoided by having partitioning variables that are constant over time (and also within families, in this instance). The level of the parameter stability tests do not really avoid that. 

To aid interpretation, we would like to retain large enough subgroups. We therefore specified the \texttt{minsize} argument, so that splits will only be implemented if the resulting nodes contain at least 250 observations. With a total sample size of 1433, even small differences in the effects of age and Head Start participation between subgroups may become significant. The average number of measurements per family was 5.6. A minimum node size of 250 ensured that the estimated effects of age and Head Start participation in the terminal nodes would at least be based on data from about 50 families.


\subsubsection{Interpreting a GLMM tree}

Next, we plot the tree. With multiple fixed-effects predictors of interest, the default plots may become too crowded or difficult to interpret. We therefore specify \verb|type = "simple"| to facilitate interpretation, and using the \verb|nodesize_level| argument, we specified that the sample size printed above every terminal node should count the number of children, instead of the number of individual observations, which would be printed by default:

\begin{Schunk}
\begin{Sinput}
 plot(HS_tree, type = "simple", nodesize_level = 2)
\end{Sinput}
\end{Schunk}


\begin{table}

\caption{\label{tab:predictions}Node-specific predicted PPVT scores at different ages.}
\begin{tabular}[t]{cccc}
\toprule
Node & Program & PPVT at age 6 & PPVT at age 12\\
\midrule
2 & None & 45.70 & 95.63\\
2 & HS & 49.48 & 98.02\\
5 & None & 50.36 & 107.43\\
5 & HS & 53.74 & 104.98\\
6 & None & 53.33 & 107.82\\
6 & HS & 59.99 & 115.70\\
7 & None & 59.64 & 117.13\\
7 & HS & 63.56 & 124.09\\
\bottomrule
\multicolumn{4}{l}{\textsuperscript{} extit\{Note.\} For computing predictions, random}\\
\multicolumn{4}{l}{effects were assumed zero. HS = Head Start; PPVT =}\\
\multicolumn{4}{l}{Peabody Picture Vocabulary Test.}\\
\end{tabular}
\end{table}
\FloatBarrier

\begin{figure}%
\caption{GLMM tree with trajectories of PPVT scores}
\includegraphics{_MOB_paper-015}

\vspace*{-3cm}

\includegraphics{_MOB_paper-016}
\\\textit{Note}: Red lines depict average node-specific trajectories for siblings participating in Head Start; black lines depict average node-specific trajectories for siblings not participating in Head Start.
\label{fig:lmm_tree}
\end{figure}%

The resulting tree is presented in Figure~\ref{fig:lmm_tree}. Below each terminal node, we plotted the observations and the two regression curves given by the coefficients in that node. The first split was made based on the AFTQ variable, which represents the mother's score on the Armed Forced Qualification Test, adjusted for the age at which they completed the test. The group with higher mother's AFTQ scores is further split based on race. The Black and Hispanic group is further split based on mother's AFTQ score. 

To aid interpretation of the coefficients in Figure~\ref{fig:lmm_tree}, Table~\ref{tab:predictions} provides predicted PPVT scores for each of the groups and programs at ages 6 and 12. All nodes show a modest benefit of Head Start participation at age 6, about 3-4 points on the PPVT. This benefit increases over time for the group with higher mother's AFTQ scores (nodes 6 and 7). The benefit decreases over time for children in the group with lowest mother's AFTQ scores (node 2). For Black and Hispanic children with intermediate mother's AFTQ scores (node 5), the benefit at age 6 even shifts to a disadvantage of Head Start participation at age 12. 

These results partly correspond to the findings of \cite{Demi09}, who report a relative benefit of Head Start participation of about 0.15 standard deviations at age 5-6, which fades out over time to about 0.05 standard deviations at ages 11-14. Fade out was reported to be particularly strong for African-American and very disadvantaged children. In the current sample, the standard deviation of PVVT scores was 34.35, and thus the predictions for age 6 in Table~\ref{tab:predictions} roughly correspond to the effect found by \cite{Demi09}. However, we found a different pattern of fade-out.

It should be noted that MOB and GLMM trees are exploratory techniques: The subgroup structures are detected from the data. Such data-driven procedures cannot provide valid standard errors and significance tests to evaluate significance of the observed subgroup differences. To that end, the subgroup structure that was found should be used in a confirmatory analysis on a new sample, that was not used to fit the tree. This would allow to test the hypothesis, either using frequentist or Bayesian approaches, of whether the parameters of interest really differ between the detected subgroups. 

If additional data is not available or cannot be collected, another possibility would be to split a single dataset into two parts before the analyses. Then one part of the dataset can be used for exploration (i.e., detecting the subgroups), and the other part can be used to test the parameter differences between the subgroups. 

%To evaluate whether the detected subgroups indeed contribute to better predictions, we used cross validation. We \edc{randomly} separated the 258 families in the dataset into ten equally-sized folds. We took nine of the ten folds as a training dataset on which we fitted two models: an LMM tree \edc{like the one shown above} (i.e., an LMM with subgroups \todo{necessary? confused me}) and an LMM without subgroups (comprising main and interaction effects of age and Head Start participation, and a random intercept with respect to child). We evaluated performance on the remaining folds by computing and evaluating accuracy of the predictions. \todo{explain MSE and $R^2$} We repeated this procedure ten times, so that all folds were used as a test set once. The results are presented in Table~\ref{tab:performance}, which shows that LMM trees generalize well: They provide better predictive accuracy compared to LMMs, while implementing only few splits. 








\section{MOB for Detecting DIF in Measurement Models}
\label{sec:raschtree}

Validity assessment of scores on psychological or educational tests requires researchers to assess whether the same construct is measured in the same way for different groups. In the framework of IRT and Rasch measurement, test items are typically investigated with respect to item misfit, multidimensionality and other violations of the measurement model \citep[cf., for example][for an introduction]{DebStrZei:2022:CRC}. A particularly crucial assumption for the comparability of test scores between groups is measurement invariance. Items that violate measurement invariance by showing different measurement properties for different groups of participants exhibit differential item functioning (DIF).

MOB can be used to detect DIF by means of testing whether the item parameters of the measurement model exhibit significant instability with respect to (combinations of) covariates. For the Rasch measurement model, the \texttt{R} function for conducting the MOB analysis is the \texttt{raschtree} function from package \texttt{psychotree} \citep{StroyKopf15}. The usage of this function will be illustrated in the next section (\hyperref[sec:TutorialRasch]{Application Example 2}). We will see that, just like for GLMM trees, we need to specify which variables are part of the parametric model. In the case of the Rasch model, this will be the test items. Moreover, we need to specify which covariates are made available to the MOB algorithm for selecting relevant splitting variables and cutpoints. 

If one joint Rasch model holds for the entire sample, that is, if there is no DIF, a Rasch tree should show no splits. However, in certain settings in educational research, such as large scale assessments, very large sample sizes are available for testing for DIF. Large sample sizes are good for detecting even small effects or model violations with high statistical power. The same holds for the statistical tests used for detecting parameter change in the MOB algorithm, so that in larger samples even very small parameter differences can be detected. However, in DIF detection this may mean that even very small DIF effects, that in practice can be considered ignorable, will be detected if only the sample is big enough. %As we will illustrate in the first part of \hyperref[sec:TutorialRasch]{Application Example 2}, for a large sample of university students who have taken a quiz to test their general knowledge, this can lead to very large trees, that are hard to interpret and contain splits that would not be considered relevant by measurement experts. 
Therefore, an extension of Rasch trees has been suggested by \citet{HennyDeba23} based on the Mantel-Haenszel effect size measure for DIF. \citet{HolTha:1985} have suggested an intuitive classification of DIF effect sizes based in the Mantel-Haenszel statistic, that is being widely used in educational testing. In this classification, category A stands for negligible DIF (small effect size or not statistically significant), B for medium DIF (neither A nor C), and C for large DIF (large effect size and statistically significant). \citet{HennyDeba23} have incorporated this classification as an additional stopping criterion for Rasch trees, so that the user can decide, for example, that a split should only be conducted if the detected DIF is of category B or C, while negligible DIF of category A should be ignored. 

Together with a purification step \citep[see][for details]{HennyDeba23}, the Mantel-Haenszel classification can also be used for graphically highlighting those items that show DIF with respect to certain groups of persons. This can help generate hypotheses about the sources of DIF, and can also aid in deciding how to proceed with the DIF items. 
%\todo[inline]{following text from Caro can be used here or later after example 2 or in discussion}
For example, items that show DIF between different language groups may often be improved by means of making sure that in all translations the meaning is as similar as possible, and/or that the words employed in the translations for the different languages are equally frequently used. In other situations, sources of DIF might be harder to eliminate, so that often DIF items may be excluded from a test. Either way, the measurement model needs to be refitted and its assumptions checked again after the final set of items has been decided upon and, in the case of modified items, administered again. 

Another important aspect to keep in mind is that DIF can be caused by one or more items measuring a secondary dimension in addition to the dimension that is intended to be measured by the test. This can be the case, for example, for an instrument intended to measure math aptitude, that contains both pure algebra problems as well as story problems. Students whose native language is not the same as the test language can have a disadvantage in answering the story problems, for example when these contain seldomly used words. These items may then show DIF between native and non-native speakers. When encountering this, the test developers will have to decide whether the items with story problems should be excluded from the test, whether they can be improved, for example by using more frequently used words, or whether the test should be considered two- rather than one-dimensional. For a discussion of the connection between DIF and multidimensionality, see also \cite{Ack:1992} and \cite{Stretal:2021:APM}.

%We will now illustrate how to use the \texttt{R} packages \texttt{glmertree} and \texttt{psychotree} to conduct the MOB analyses. 


\subsection{Application Example 2: Detecting DIF in a General Knowledge Quiz}
\label{sec:TutorialRasch}


\subsubsection{Dataset}

We will illustrate the usage of Rasch trees and the Mantel-Haenszel effect size measure by means of a dataset from \citet{SPISA:book}. An online general knowledge quiz was conducted by the German
weekly news magazine SPIEGEL. Below we will use the abbreviation \texttt{SPISA} for the name of the data set, because the quiz was termed ``students' PISA'' by the magazine, but it is not related to the ``real'' PISA study by the OECD. The data set contains the quiz results as well as sociodemographic data. In the following we will use only the responses to the nine items of the \textit{culture} scale (i.e., items 28 through 36) from test booklet 20 and only the $9769$ complete cases of university students for illustration. The wording of the items is provided in the \hyperref[sec:AppendixA]{Appendix}.

Three possible partitioning variables are considered: \texttt{Gender} (in this data set coded only as male, female, or missing), \texttt{Age} (continuous), and \texttt{Area} (Area of study: Language \& Culture, Law \& Economics, Medicine \& Health, Engineering, Sciences, Pharmacy, Geography, Agriculture \& Nutrition, or Sports). The nine items from the culture scale are scored with 0 (incorrect answer) or 1 (correct answer). 
The quiz data have been prepared in a format where the nine items of the culture scale are not stored as individual columns of the data set, but the complete matrix of item responses is stored as a single variable \citep[see][]{raschtree:vignette} under the name \texttt{culture}. This means that we can access the item responses for all items jointly using the name \texttt{culture} in the following \texttt{R} commands. The first few rows of the data set look like this:

\begin{Schunk}
\begin{Sinput}
 load("dat_SPISA.Rda")
 head(dat_SPISA, 3)
\end{Sinput}
\begin{Soutput}
    Age Gender                    Area culture.i28 culture.i29 culture.i30
90   21   Male       Law and Economics           1           1           0
163  25   Male          no Information           1           1           1
402  22   Male Agriculture & Nutrition           0           1           0
    culture.i31 culture.i32 culture.i33 culture.i34 culture.i35 culture.i36
90            1           0           1           1           1           1
163           1           0           1           1           0           1
402           1           1           0           1           1           0
\end{Soutput}
\end{Schunk}

\subsubsection{Fitting a Rasch tree}

The results in \citet{SPISA:book} indicate that the Rasch model is appropriate for the individual scales of the general knowledge quiz, so that we can continue to fit Rasch trees by means of function \texttt{raschtree} from the R package \texttt{psychotree}. Note that model-based recursive partitioning is also available in \texttt{psychotree} for other IRT models.

The model which we would like to test for parameter stability in this example, the Rasch model, is based only on the matrix of item responses \texttt{culture}. It does not contain predictor variables of its own. Therefore, the formula syntax for the \texttt{raschtree} function is simpler than that for the \texttt{lmertree} function in \hyperref[sec:TutorialMixed]{Application Example 1}: On the left hand side of the \verb|~| symbol, we provide the item response matrix \texttt{culture} for the Rasch model. On the right hand side of the \verb|~| symbol, we provide the possible partitioning variables \texttt{Gender}, \texttt{Age}, and \texttt{Area}.



\begin{Schunk}
\begin{Sinput}
 library("psychotree")
 Raschtree_culture <- raschtree(culture ~  Gender + Age + Area,
                                data = dat_SPISA)
\end{Sinput}
\end{Schunk}


Once the Rasch tree has been fitted, we can plot it using the standard \texttt{plot} function: 

\begin{Schunk}
\begin{Sinput}
 plot(Raschtree_culture)
\end{Sinput}
\end{Schunk}

\begin{figure}%
\caption{Rasch tree fitted to the SPISA quiz items using the default stopping criterion based on statistical significance.}
\begin{subfigure}{1.2\textwidth}
\includegraphics{_MOB_paper-023}
\end{subfigure}
\label{fig:MHtree1}
\end{figure}%

The resulting tree is depicted in Figure \ref{fig:MHtree1}. The item parameters for the nine culture items are displayed in the terminal nodes of the tree. However, it is hard to interpret the tree structure because such a large number of splits were created. Moreover, it is hard to tell whether all splits are due to substantial amounts of DIF, or whether some of the splits are due to small DIF effects that only became statistically significant due to the large sample size.

To assess whether DIF effects are substantial or negligible, it is possible to use the Mantel-Haenszel DIF effect size measure as an additional stopping criterion. It has three categories (A: negligible, B: moderate, C: large). If none of the items shows DIF in category B or C\footnote{This is the default setting. It can also be changed so that splitting is stopped if no item shows DIF in category C.}, the tree is stopped from growing further.
At the moment, the \texttt{R} functions for the Mantel-Haenszel stopping criterion and additional visualizations based on the Mantel-Haenszel effect size are available from GitHub. They will be made available as part of the \texttt{psychotree} package in the future.

In order to install the functions from the author's GitHub page and make them available in the current \texttt{R} session, use the following two commands:

\begin{Schunk}
\begin{Sinput}
 library(devtools)
 install_github("mirka-henninger/raschtreeMH")
 library("raschtreeMH")
\end{Sinput}
\end{Schunk}

We can use a syntax very similar to that of the original \texttt{raschtree} function, but now we also specify the additional argument \texttt{stopfun}. We want to use the Mantel-Haenszel stopping function (\texttt{stopfun\_mantelhaenszel}) together with iterative purification. It is also possible to provide other, user-defined stopping functions \citep[see][for details]{HennyDeba23}.

For the purification strategy, there are three options: \texttt{none}, \texttt{2step} and \texttt{iterative}. Purification means that items which have already been diagnosed with DIF are taken out of the sum score, which is used as the matching criterion in the final DIF test. It is highly recommended to use a purification strategy, because otherwise false positives (i.e., artificial DIF) may occur \citep[cf., e.g.,][and the references therein]{DebStrZei:2022:CRC,HennyDeba23,Kopfetal:2015:EPM}. In two-step purification, a new sum score without the items that were diagnosed with DIF in step 1 is computed for the final DIF analysis in step 2. When using iterative purification, this process is repeated until two runs yield the same DIF items or until the maximum number of iterations is reached. 
The final set of DIF-free items resulting from the purification will also be used for aligning the item parameters for each group comparison, which we will discuss and display below.

We fit the Raschtree using the Mantel-Haenszel stopping criterion as follows:


\begin{Schunk}
\begin{Sinput}
 Raschtree_MH_culture <- raschtree(culture ~  Gender + Age + Area,
                                   data = dat_SPISA,
                                   stopfun= stopfun_mantelhaenszel(
                                     purification = "iterative"))
\end{Sinput}
\end{Schunk}

For technical reasons, the information about the Mantel-Haenszel effect size and classification is not saved in the tree object itself, but has to be added afterwards using the \texttt{add\_mantelhaenszel} function.

\begin{Schunk}
\begin{Sinput}
 Raschtree_MH_culture <- add_mantelhaenszel(Raschtree_MH_culture,
                                            purification = "iterative")
\end{Sinput}
\end{Schunk}


\subsubsection{Interpreting a Rasch tree}


\begin{figure}%
\caption{Rasch tree fitted to the SPISA quiz items based on using Mantel-Haenszel DIF effect size measure as the stopping criterion.}
\includegraphics{_MOB_paper-028}
\label{fig:MHtree2}
\end{figure}%

After we have added this information to the Rasch tree, we can now visualize it with additional features: 

\begin{Schunk}
\begin{Sinput}
 plot(Raschtree_MH_culture)
\end{Sinput}
\end{Schunk}

The result is presented in Figure~\ref{fig:MHtree2}, which reveals that the Rasch tree is much more concise with a lower number of splits. In addition, in each node the number of items classified as A, B, or C is displayed. For instance, we can see that in node 1 one item is classified in category B (moderate DIF), and four items are classified in category C (large DIF).

We can also color the DIF items in the terminal node profiles according to a particular split in the tree. We will illustrate this with two examples: First, we can see that for node 1 (split in the covariate \texttt{Gender}), items 1, 6, 7, 8, and 9 show DIF in categories B or C. As displayed in the \hyperref[sec:AppendixA]{Appendix}, the content of these items was:

\begin{itemize}
\setlength\itemsep{0em}
\item 1: Painting by Andy Warhol
\item 6: Novel by Daniel Kehlmann: Die Vermessung der Welt
\item 7: City of the Buddenbrooks: Lübeck
\item 8: City with building: Paris
\item 9: Opera not by Mozart: Aida
\end{itemize}

\begin{Schunk}
\begin{Sinput}
 plot(Raschtree_MH_culture, color_by_node = 1)
\end{Sinput}
\end{Schunk}

\begin{figure}%
\caption{Rasch tree fitted to the SPISA quiz items based on using Mantel-Haenszel DIF effect size measure as the stopping criterion with additional visualization features.}
\includegraphics{_MOB_paper-031}
\label{fig:MHtree3}
\end{figure}%

These items are %(relative to the DIF-free items for this group comparison) 
easier or less difficult to answer for participants with female or missing gender (node 7), than for males (nodes 3, 5 and 6). Alternatively, we can also color the DIF items in the terminal node profiles according to a later split in the tree:


% We can also color the DIF items according to node 2 (split in the covariate \texttt{Age}). Again, item 1 (Painting by Andy Warhol) shows DIF in category B. It is (relative to the DIF-free items for this group comparison) more difficult to answer for younger respondents (age $\leq 24$) compared to older respondents.

% <<fig = TRUE, width = 9, height = 7>>=
% plot(Raschtree_MH_culture, color_by_node = 2)
% @


\begin{Schunk}
\begin{Sinput}
 plot(Raschtree_MH_culture, color_by_node = 4)
\end{Sinput}
\end{Schunk}


\begin{figure}%
\caption{Rasch tree fitted to the SPISA quiz items based on using Mantel-Haenszel DIF effect size measure as the stopping criterion with additional visualization features.}
\includegraphics{_MOB_paper-033}
\label{fig:MHtree4}
\end{figure}%

The result is presented in Figure~\ref{fig:MHtree4}. For node 4 (split in the covariate \texttt{Age}), items 1 (Painting by Andy Warhol) and 9 (Opera not by Mozart: Aida) show DIF and are %(relative to the DIF-free items for this group comparison) 
more difficult to answer for younger male respondents ($24 <$ age $\leq 29$), and easier or less difficult to answer for older male respondents (age $>29$).

Detailed information about the Mantel-Haenszel criterion for each item in each node can also be accessed from the Rasch tree object using \texttt{\$info\$mantelhaenszel}. It contains the DIF classification and the value of the Mantel-Haenszel effect size measure for each item in each node, as well as the type of purification that was employed in each node, and in the case of iterative purification how many iterations were conducted.

\begin{Schunk}
\begin{Sinput}
 Raschtree_MH_culture$info$mantelhaenszel
\end{Sinput}
\begin{Soutput}
$classification
      node4 node2 node1
item1 "B"   "B"   "C"  
item2 "A"   "A"   "A"  
item3 "A"   "A"   "A"  
item4 "A"   "A"   "A"  
item5 "A"   "A"   "A"  
item6 "A"   "A"   "C"  
item7 "A"   "A"   "C"  
item8 "A"   "A"   "B"  
item9 "B"   "A"   "C"  

$mantelhaenszel
            node4       node2        node1
item1 -1.53377668 -1.03559992 -1.802977840
item2  0.72945873 -0.14242027  0.377084860
item3 -0.35893502  0.51100000  0.127575191
item4 -0.05897255  0.40879442 -0.007385896
item5  0.41615400 -0.08996266  0.128463797
item6  0.42418247 -0.16529252 -1.875191790
item7 -0.87987154 -0.57949560 -1.620265527
item8  0.69240041  0.54735808 -1.083093749
item9 -1.00725807 -0.39502640 -1.798520587

$purification
      node4       node2       node1 
"iterative" "iterative" "iterative" 

$purification_counter
node4 node2 node1 
    1     1     2 
\end{Soutput}
\end{Schunk}

The displayed information again shows that some items show only negligible DIF (category A) in all nodes, while other items show medium (category B) or large (category C) DIF in some or all nodes. The values of the Mantel-Haenszel effect size measure are negative when the item difficulty is lower in the right than in the left child node. For example, as we already observed in Figure~\ref{fig:MHtree3}, items 1, 6, 7, 8, and 9 have medium or large DIF (DIF categories B and C) and are  easier to answer %(relative to the DIF-free items for this group comparison) 
for participants with female or missing gender (node 7). % Item 4 also has a negative sign, indicating that the item difficulty is lower for participants with female or missing gender. However, this item has a smaller DIF effect size. Thus it falls into DIF category A (negligible) and was not relevant for the split in \texttt{Gender}.


\FloatBarrier
\section{Discussion}
\label{sec:discussion}

In this tutorial paper we have outlined the rationale of the MOB algorithm and how it can be used for detecting parameter differences in mixed-effects and Rasch models. The main advantage of MOB is that it can detect groups of persons with different model parameters in a data-driven manner. This makes it more flexible for detecting differences that were not hypothesized by the researcher. For example, it is often the case that obvious sources of DIF have already been avoided by the content experts during item creation. Any remaining DIF is unexpected. Therefore, DIF detection approaches that rely on the researcher to correctly specify the exact groups of persons that exhibit DIF can miss DIF, because it may be associated with other (combinations of) covariates than the ones investigated. In this sense, the more exploratory approach of Rasch trees has higher statistical power to detect DIF in previously unknown groups \citep{StroyKopf15}. The same argument holds for parameter differences in mixed-effects models, where the substantial hypotheses formulated a priori are typically about global intervention effects or global effects of time, and not about possible subgroup-specific differences. GLMM trees allow for identifying possible interactions the predictor of interest may be involved in.

However, researchers should be aware that a covariate-based approach like MOB will only be able to detect parameter differences if the relevant covariates were recorded and supplied to the algorithm. If no (or few) potentially relevant covariates are available, an alternative framework for detecting previously unknown groups of persons with different model parameters is mixture modelling \citep[see, e.g.,][in the context of Rasch modelling]{AyalySant17,FriStrZei:2015:EaPM}. Mixture modelling aims at identifying latent classes of persons with different properties. It can also be combined with observed covariates. A comparison of MOB and mixture modelling is given by \citet{FriStrZei:2014}.

Another important caveat is that those covariates that are selected for splitting by the MOB algorithm are not necessarily causal for the observed parameter differences. They might also be proxies for other, unobserved covariates. For example, if DIF in the item parameters of a test on reading ability is found between different cities or districts, a variety of factors could be causally driving these effects, such as different compositions of native languages or sociodemographics. On the other hand, MOB can also be useful in evaluating causal effects, as it can be used to test for possible confounding \citep{vanWyLi19}.

As with any exploratory method, MOB should be considered as a tool for generating hypotheses and not as a confirmatory technique. While the MOB algorithm, as it is implemented in the \texttt{R} packages presented in this tutorial, has been constructed with care and uses statistical significance tests (and in the case of Mantel-Haenszel trees also effect sizes) to avoid spurious splits, for confirmatory hypothesis testing it is recommended to use a new dataset, not used for fitting the tree. 

For example, the results of the GLMM tree analysis (\hyperref[sec:TutorialMixed]{Application Example 1}) could be validated using later cohorts from the same study. On data from later cohorts, a confirmatory mixed-effects model could be fitted, similar to the one depicted in Figure~\ref{fig:global_lmm}, but with additional binary indicators distinguishing the subgroups. The model could be specified to include main effects as well as interactions with time and treatment (i.e., Head Start versus no Head Start). This refitting on new data will yield valid standard errors, and thus provide hypothesis tests on subgroup differences. Alternatively, if the interest of the analyses was primarily to identify moderators of the effect of Head Start, the model fitted on a later cohort could comprise main and interactions effects of time, Head Start, Mother's AFTQ score and Race. 

Identified subgroups can also guide the design of future studies. For example, if subgroups with differential treatment effects were identified by a GLMM tree in one randomized clinical trail (RCT), a next RCT may be designed so that it has adequate power to establish the differential subgroup effects with a confirmatory analysis. Or, after developing a new quiz, or improving the quiz analysed in \hyperref[sec:TutorialRasch]{Application Example 2}, a validation study may be conducted so as to assess DIF between the subgroups identified with a Rasch tree in an earlier analysis. This would allow to assess whether the DIF has been mitigated in the new or improved quiz.

As mentioned in \hyperref[sec:TutorialMixed]{Application Example 1}, resampling or $k$-fold cross validation techniques may be used to assess stability and accuracy of the GLMM tree. \cite{PhilyRusc18} show how to use these techniques for assessing the stability of selected variables and cutpoints. For GLMM trees, cross-validation can additionally be used for evaluating predictive accuracy \citep[e.g., ][]{RooiyWeed20}, or for further optimizing predictive accuracy through tuning model-fitting parameters, like the minimum number of observations to be retained in terminal nodes or the maximum tree depth.  

%\todo[inline]{I (Caro) have added a few more thoughts for the discussion above - Marjolein maybe this is already sufficient in length? please feel free to use anything from above or below and get back to me if you add/keep points where we might disagree}

%Due to the exploratory nature of MOB, it does not allow for hypothesis tests\todo[inline]{I (Caro) think this will sound confusing because earlier in the paper we explain that MOB is indeed based on significance tests, and due to the fact that the stopping is based on the significance test and (at least in party and psychotree) we use Bonferroni for adjusting against the number of covariates, it is not like we are doing completely uncontrolled multiple testing - in the Raschtree paper we write about this: Moreover, it is important to note that our model-based recursive partitioning algorithm is not affected by an inflation of chance due to its recursive nature. Indeed, several statistical tests are successively conducted in a Rasch tree—but each test is conducted only if the previous test yielded a significant result. In this sense, the recursive approach forms a closed testing procedure, which does not lead to an inflation of chance as is well known from the literature on multiple comparisons (Marcus, Peritz, \& Gabriel, 1976; Hochberg \& Tamhane, 1987). For the Rasch tree, this means that the postulated significance level holds for the entire tree, not only for each individual split. This ensures that DIF is not erroneously detected as an artifact of the recursive nature of the algorithm. -- another, related, limitation that I would agree with and might be what you also had in mind is that the tree will not necessarily find the globally best partition, because every split is based on the previous split choices, here is what we wrote about this in the tree intro PsychMeth paper: Thus, variable selection in a single tree is affected by order effects similar to those present in stepwise variable selection approaches for parametric regression (that is also instable against random variation of the learning data, as pointed out by Austin and Tu 2004). In both recursive partitioning and stepwise regression, the approach of adding one locally optimal variable at a time does not necessarily (or rather hardly ever) lead to the globally best model over all possible combinations of variables. -- should we write something similar here?}, as there is no valid way to account for the exploratory searching of the subgroups. Thus, even if detected subgroups are substantively meaningful or differences are large, if researchers want to ascertain statistical significance of the subgroup differences, this should be done on new data using confirmatory techniques. \todo{Marjolein write a little more here? e.g. such as cross validation.}

\hyperref[sec:TutorialRasch]{Application Example 2} showed that the Rasch tree method can employ an effect size measure to stop the tree from growing if effect-size differences are non-substantial. This also supports researchers in interpreting the tree's results with respect to whether DIF effects are negligible, medium, or large. For mixed-effects models, computation of effect sizes is much less straightforward and several different approaches have been proposed \citep[e.g., ][]{JuddyWest17, BrysyStev18, Bran18}. Effect-size based stopping criteria for GLMM trees are therefore currently not available, but in principle, any effect size that can be computed for traditional GLMMs can also be computed for GLMM trees.


%\todo{Interpretation methods might not work as expected for multilevel data, so I am not convinced we should discuss this and call it a remedy.} 
%A remedy to this issue can be interpretation techniques, such as partial dependence plots or individual conditional expectations plots. These interpretation techniques support researchers in gauging the size of the effect of predictor variables visually by depicting the predicted value of the machine learning method as a function of the value of the predictor variable(s). A comprehensive introduction, tutorial, and discussion into interpretation techniques for machine learning methods is given by \cite{Molnar2019} and \cite{Henninger2022c}. 

For GLMM trees, the use of cluster-level partitioning variables would commonly be desired in longitudinal analyses. As with traditional GLMMs, specification of the fixed and random effects requires careful consideration. For example, whether random slopes of time should be estimated, or whether random intercepts are sufficient to account for dependency between observations. Building on recent work by \citet{FokkyZeil24}, we will further study and develop methods to take various dependence structures into account. For instance, in future implementations of \texttt{glmertree}, parameter stability tests that allow for detecting subgroup differences in the random-effects parameters \citep{Wang2021a} may be implemented.

%\todo{I (Mirka) added a short point on future work regarding cluster-level predictors and their Type-I error rate here. $\rightarrow$ this paragraph can also transition well to your comment on including parameter stability tests for random-effects parameter \color{black}Mention that the exclusion of random effects from partitioning can both be advantage and disadvantage. In future work, use of parameter stability tests for random-effects parameters of Ting and Ed will be explored.}



\bibliography{_MOB_paper}

%\todo{Add DOIs.}

\newpage
\appendix



%\section{Association between Age and PPVT scores before and after transformation}
%\label{sec:AppendixA}
%
%
%\begin{figure}[h!]
%\caption{Association between Age and PPVT scores before (left) and after (right) transformation.}
%<<echo=FALSE, fig=TRUE, width=6, height=4>>=
%par(mfrow = c(1, 2))
%plot(HS_dat$Age, HS_dat$PPVT, cex = .7, xlab = "Age", ylab = "PPVT",
%     cex.lab = .7, cex.axis = .7)
%plot(sqrt(HS_dat$Age), HS_dat$PPVT, ylab = "PPVT", xlab = "Age (square root)", 
%     cex = .7, cex.lab = .7, cex.axis = .7)
%@
%\end{figure}


\section{Items of the culture scale for \hyperref[sec:TutorialRasch]{Application Example 2}}
\label{sec:AppendixA}

\begin{enumerate}
\item Which painter created this painting? – Andy Warhol.
\item What do these four buildings have in common? – All four were designed by the same architects.
\item Roman numbers: What is the meaning of CLVI? – 156.
\item What was the German movie with the most viewers since 1990? – Der Schuh des Manitu.
\item In which TV series was the US president portrayed by an African American actor for a long time? – 24.
\item What is the name of the bestselling novel by Daniel Kehlmann? – Die Vermessung der Welt (Measuring The World).
\item Which city is the setting for the novel ‘Buddenbrooks’? – Lübeck.
\item In which city is this building located? – Paris.
\item Which one of the following operas is not by Mozart? – Aida.
\end{enumerate}


\end{document}


\begin{Schunk}
\begin{Soutput}
Writing to file _MOB_paper.R 
\end{Soutput}
\end{Schunk}
